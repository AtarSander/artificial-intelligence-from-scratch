{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raport z zadania sztuczne sieci neuronowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import MnistDataloader\n",
    "from experiment import experiment, plot_costs\n",
    "from model import Model\n",
    "from layer import Layer\n",
    "import numpy as np\n",
    "from PIL import Image as im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader = MnistDataloader(\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\", \"t10k-images.idx3-ubyte\", \"t10k-labels.idx1-ubyte\")\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image(image):\n",
    "    image = np.array(image)\n",
    "    data = im.fromarray(image)\n",
    "    data.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.add_module(Layer((784, 256), \"relu\"))\n",
    "model.add_module(Layer((256, 128), \"relu\"))\n",
    "model.add_module(Layer((128, 64), \"relu\"))\n",
    "model.add_module(Layer((64, 10), \"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, epochs, learning_rate, batch_size):\n",
    "    dataset_size = len(X)\n",
    "    for epoch in range(epochs):\n",
    "        # print(f\"Epoch: {epoch}\")\n",
    "        for i in range(0,dataset_size,batch_size):\n",
    "            # print(f\"batch: {i}\")\n",
    "            batch_X = np.array(X[i:i+batch_size]) / 255.0\n",
    "            batch_Y = np.array(y[i:i+batch_size])\n",
    "            loss = model.step(batch_X, batch_Y, learning_rate)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, label):\n",
    "    pred = model.predict(np.array(X))\n",
    "    return f\"Prediction: {pred}, Label: {label}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.03843716743479128\n",
      "Epoch: 10, loss: 0.02275984614788316\n",
      "Epoch: 20, loss: 0.02079451024686997\n",
      "Epoch: 30, loss: 0.01828783820998918\n",
      "Epoch: 40, loss: 0.01045114631347432\n",
      "Epoch: 50, loss: 0.0013841048134248428\n",
      "Epoch: 60, loss: 0.0004083700946705055\n"
     ]
    }
   ],
   "source": [
    "train(x_train, y_train, 100, 3e-4, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "device = torch.device(\"cuda\"if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.linear2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.linear3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.linear4 = nn.Linear(in_features=64, out_features=32)\n",
    "        if device.type == \"cuda\":\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_list, img_array, transform=None, target_transform=None):\n",
    "        self.img_labels = annotations_list\n",
    "        self.images = img_array\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(dataset, model, learning_rate, epochs, batch_size):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for image, label in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model((image/ 255.0).to(device))\n",
    "            loss = criterion(y_pred, label.to(device))\n",
    "            print(f\"Loss:{loss}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch:{epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MnistDataset(np.array(y_train), np.array(x_train))\n",
    "torch_model = TorchModel(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:7.183226585388184\n",
      "Loss:5.526722431182861\n",
      "Loss:5.632143020629883\n",
      "Loss:4.806131362915039\n",
      "Loss:4.688581943511963\n",
      "Loss:4.7320780754089355\n",
      "Loss:4.388777732849121\n",
      "Loss:4.280552387237549\n",
      "Loss:3.8892598152160645\n",
      "Loss:4.133275985717773\n",
      "Loss:3.5533838272094727\n",
      "Loss:3.6376161575317383\n",
      "Loss:3.6535890102386475\n",
      "Loss:3.619192123413086\n",
      "Loss:3.247619390487671\n",
      "Loss:3.2427988052368164\n",
      "Loss:3.1567389965057373\n",
      "Loss:3.2508623600006104\n",
      "Loss:3.12253999710083\n",
      "Loss:3.07212233543396\n",
      "Loss:2.838172197341919\n",
      "Loss:2.819213628768921\n",
      "Loss:2.9339609146118164\n",
      "Loss:2.758969783782959\n",
      "Loss:2.686230182647705\n",
      "Loss:2.6710498332977295\n",
      "Loss:2.5008907318115234\n",
      "Loss:2.6712229251861572\n",
      "Loss:2.5591840744018555\n",
      "Loss:2.8358194828033447\n",
      "Loss:2.4263806343078613\n",
      "Loss:2.398364305496216\n",
      "Loss:2.4989407062530518\n",
      "Loss:2.1339433193206787\n",
      "Loss:2.5319674015045166\n",
      "Loss:2.1941096782684326\n",
      "Loss:2.3115954399108887\n",
      "Loss:2.4435923099517822\n",
      "Loss:2.187356472015381\n",
      "Loss:2.3480963706970215\n",
      "Loss:2.1209006309509277\n",
      "Loss:2.0830483436584473\n",
      "Loss:1.9194570779800415\n",
      "Loss:1.940250039100647\n",
      "Loss:1.9316866397857666\n",
      "Loss:2.0452444553375244\n",
      "Loss:1.84260892868042\n",
      "Loss:1.571102499961853\n",
      "Loss:1.8967647552490234\n",
      "Loss:1.7979180812835693\n",
      "Loss:1.858161449432373\n",
      "Loss:1.6908283233642578\n",
      "Loss:1.948098063468933\n",
      "Loss:1.951242208480835\n",
      "Loss:1.8879083395004272\n",
      "Loss:1.8074977397918701\n",
      "Loss:1.868988037109375\n",
      "Loss:1.864917516708374\n",
      "Loss:1.8677810430526733\n",
      "Loss:1.8771405220031738\n",
      "Loss:1.621315598487854\n",
      "Loss:1.9437141418457031\n",
      "Loss:1.724364995956421\n",
      "Loss:1.517279863357544\n",
      "Loss:1.666751503944397\n",
      "Loss:1.594641089439392\n",
      "Loss:1.6976864337921143\n",
      "Loss:1.6467959880828857\n",
      "Loss:1.5908998250961304\n",
      "Loss:1.7886310815811157\n",
      "Loss:1.5151454210281372\n",
      "Loss:1.638909935951233\n",
      "Loss:1.5386803150177002\n",
      "Loss:1.5574625730514526\n",
      "Loss:1.470226526260376\n",
      "Loss:1.3010462522506714\n",
      "Loss:1.53867506980896\n",
      "Loss:1.3778291940689087\n",
      "Loss:1.3157544136047363\n",
      "Loss:1.2902534008026123\n",
      "Loss:1.3092544078826904\n",
      "Loss:1.0070641040802002\n",
      "Loss:1.0567984580993652\n",
      "Loss:1.2034541368484497\n",
      "Loss:1.245116114616394\n",
      "Loss:1.1582876443862915\n",
      "Loss:1.2428866624832153\n",
      "Loss:1.3153390884399414\n",
      "Loss:1.1561537981033325\n",
      "Loss:1.1989415884017944\n",
      "Loss:1.5241546630859375\n",
      "Loss:1.3661996126174927\n",
      "Loss:1.2463725805282593\n",
      "Loss:1.265631079673767\n",
      "Loss:1.0654809474945068\n",
      "Loss:1.3144766092300415\n",
      "Loss:1.375562071800232\n",
      "Loss:1.3046196699142456\n",
      "Loss:1.3349567651748657\n",
      "Loss:1.3367185592651367\n",
      "Loss:1.2115603685379028\n",
      "Loss:1.379549503326416\n",
      "Loss:1.2986305952072144\n",
      "Loss:1.1191309690475464\n",
      "Loss:1.1034610271453857\n",
      "Loss:1.0183472633361816\n",
      "Loss:1.0464835166931152\n",
      "Loss:1.16902756690979\n",
      "Loss:1.2185572385787964\n",
      "Loss:1.4499329328536987\n",
      "Loss:1.2775241136550903\n",
      "Loss:1.2868419885635376\n",
      "Loss:1.2276626825332642\n",
      "Loss:1.1956449747085571\n",
      "Loss:1.4698883295059204\n",
      "Loss:1.4445286989212036\n",
      "Loss:1.0006178617477417\n",
      "Loss:0.9629769325256348\n",
      "Loss:1.2161998748779297\n",
      "Loss:0.9475529193878174\n",
      "Loss:0.8684853911399841\n",
      "Loss:1.0475025177001953\n",
      "Loss:0.9959689378738403\n",
      "Loss:1.384876012802124\n",
      "Loss:1.072592854499817\n",
      "Loss:1.1611316204071045\n",
      "Loss:1.0179065465927124\n",
      "Loss:0.738508403301239\n",
      "Loss:0.9158049821853638\n",
      "Loss:0.8162673115730286\n",
      "Loss:1.0849486589431763\n",
      "Loss:1.176273226737976\n",
      "Loss:1.157070517539978\n",
      "Loss:1.010160207748413\n",
      "Loss:1.1420035362243652\n",
      "Loss:0.9796024560928345\n",
      "Loss:1.0102097988128662\n",
      "Loss:1.2057968378067017\n",
      "Loss:1.32492196559906\n",
      "Loss:1.0517085790634155\n",
      "Loss:0.9579688906669617\n",
      "Loss:0.804420530796051\n",
      "Loss:0.663645327091217\n",
      "Loss:1.1863222122192383\n",
      "Loss:1.0025635957717896\n",
      "Loss:1.0123196840286255\n",
      "Loss:0.8666189312934875\n",
      "Loss:0.7621559500694275\n",
      "Loss:0.8110678195953369\n",
      "Loss:1.0701948404312134\n",
      "Loss:0.9630576372146606\n",
      "Loss:0.9235200881958008\n",
      "Loss:0.7299878597259521\n",
      "Loss:0.5975531339645386\n",
      "Loss:0.858187198638916\n",
      "Loss:0.9719164967536926\n",
      "Loss:0.9400126338005066\n",
      "Loss:0.8549361824989319\n",
      "Loss:0.8220609426498413\n",
      "Loss:0.7659251093864441\n",
      "Loss:0.7834504246711731\n",
      "Loss:0.7946643829345703\n",
      "Loss:1.0062270164489746\n",
      "Loss:0.9496512413024902\n",
      "Loss:0.8164792060852051\n",
      "Loss:0.7150620222091675\n",
      "Loss:0.769107460975647\n",
      "Loss:0.869174599647522\n",
      "Loss:0.8067666888237\n",
      "Loss:0.7891886234283447\n",
      "Loss:0.5569398999214172\n",
      "Loss:0.5321087837219238\n",
      "Loss:0.9021697640419006\n",
      "Loss:1.0132761001586914\n",
      "Loss:0.7903534770011902\n",
      "Loss:1.029099941253662\n",
      "Loss:1.1551295518875122\n",
      "Loss:0.7800341844558716\n",
      "Loss:0.8170403242111206\n",
      "Loss:0.6505815386772156\n",
      "Loss:0.815127968788147\n",
      "Loss:0.7640226483345032\n",
      "Loss:0.7136472463607788\n",
      "Loss:0.7471678853034973\n",
      "Loss:0.8870394229888916\n",
      "Loss:0.9490110874176025\n",
      "Loss:0.863248884677887\n",
      "Loss:0.7375642657279968\n",
      "Loss:0.5646207928657532\n",
      "Loss:0.8785293698310852\n",
      "Loss:0.6704815626144409\n",
      "Loss:0.8830004930496216\n",
      "Loss:0.9851299524307251\n",
      "Loss:0.8750880360603333\n",
      "Loss:0.8545752167701721\n",
      "Loss:0.7397692203521729\n",
      "Loss:0.83051598072052\n",
      "Loss:0.9050266146659851\n",
      "Loss:0.5097472667694092\n",
      "Loss:0.642388105392456\n",
      "Loss:0.524403989315033\n",
      "Loss:0.8951879143714905\n",
      "Loss:0.8238120079040527\n",
      "Loss:0.7081436514854431\n",
      "Loss:0.655464231967926\n",
      "Loss:0.7115350365638733\n",
      "Loss:0.9828740954399109\n",
      "Loss:0.9021883606910706\n",
      "Loss:0.9200114607810974\n",
      "Loss:0.6759307384490967\n",
      "Loss:0.6439993977546692\n",
      "Loss:0.6694997549057007\n",
      "Loss:0.983368456363678\n",
      "Loss:0.7422537803649902\n",
      "Loss:0.9126206040382385\n",
      "Loss:0.588567316532135\n",
      "Loss:0.7137417197227478\n",
      "Loss:0.6908871531486511\n",
      "Loss:0.555438220500946\n",
      "Loss:0.6112912893295288\n",
      "Loss:0.7720237970352173\n",
      "Loss:0.7784655690193176\n",
      "Loss:0.7295743823051453\n",
      "Loss:0.8356872797012329\n",
      "Loss:0.6262772083282471\n",
      "Loss:0.6884726881980896\n",
      "Loss:0.6874209046363831\n",
      "Loss:0.9443842172622681\n",
      "Loss:0.611572265625\n",
      "Loss:0.6420198082923889\n",
      "Loss:0.5987276434898376\n",
      "Loss:0.7914502620697021\n",
      "Loss:0.8278701305389404\n",
      "Loss:0.8915467262268066\n",
      "Loss:0.8784888386726379\n",
      "Loss:1.0547558069229126\n",
      "Loss:0.608161449432373\n",
      "Loss:0.5825793147087097\n",
      "Loss:0.8518680930137634\n",
      "Loss:0.9071235060691833\n",
      "Loss:0.747666597366333\n",
      "Loss:0.9059325456619263\n",
      "Loss:0.7615599036216736\n",
      "Loss:0.8525334596633911\n",
      "Loss:0.9472617506980896\n",
      "Loss:0.8453592658042908\n",
      "Loss:0.7312651872634888\n",
      "Loss:1.0687025785446167\n",
      "Loss:0.7455534934997559\n",
      "Loss:0.8189125061035156\n",
      "Loss:0.661171555519104\n",
      "Loss:0.8875608444213867\n",
      "Loss:0.831480860710144\n",
      "Loss:0.8791706562042236\n",
      "Loss:0.5327625870704651\n",
      "Loss:0.6693947911262512\n",
      "Loss:0.7665755152702332\n",
      "Loss:0.5616053342819214\n",
      "Loss:0.8305230736732483\n",
      "Loss:0.6374946236610413\n",
      "Loss:0.8359099626541138\n",
      "Loss:0.8297848701477051\n",
      "Loss:0.6889629364013672\n",
      "Loss:0.5330284833908081\n",
      "Loss:0.33640527725219727\n",
      "Loss:0.4616556763648987\n",
      "Loss:0.5804249048233032\n",
      "Loss:0.49405863881111145\n",
      "Loss:0.6615365147590637\n",
      "Loss:0.7726799249649048\n",
      "Loss:0.6893777847290039\n",
      "Loss:0.747215211391449\n",
      "Loss:0.7916151881217957\n",
      "Loss:0.4831092953681946\n",
      "Loss:0.6380133032798767\n",
      "Loss:0.6457613110542297\n",
      "Loss:0.5886660814285278\n",
      "Loss:0.5826389193534851\n",
      "Loss:0.6588478088378906\n",
      "Loss:0.5757371187210083\n",
      "Loss:0.6808918714523315\n",
      "Loss:0.8409587740898132\n",
      "Loss:0.6305530071258545\n",
      "Loss:0.4413718581199646\n",
      "Loss:0.7244616150856018\n",
      "Loss:0.4759594202041626\n",
      "Loss:0.4443264305591583\n",
      "Loss:0.6472370028495789\n",
      "Loss:0.5662783980369568\n",
      "Loss:0.609960675239563\n",
      "Loss:0.6254696249961853\n",
      "Loss:0.7653011083602905\n",
      "Loss:1.078884482383728\n",
      "Loss:0.699604332447052\n",
      "Loss:0.7553534507751465\n",
      "Loss:0.7397436499595642\n",
      "Loss:0.5872997641563416\n",
      "Loss:0.5245137214660645\n",
      "Loss:0.6244034171104431\n",
      "Loss:0.7022801041603088\n",
      "Loss:0.7205820679664612\n",
      "Loss:0.7095913290977478\n",
      "Loss:0.5734535455703735\n",
      "Loss:0.4294133186340332\n",
      "Loss:0.5743480920791626\n",
      "Loss:0.4474039375782013\n",
      "Loss:0.5415765047073364\n",
      "Loss:0.9676219820976257\n",
      "Loss:0.6880576014518738\n",
      "Loss:0.5786920785903931\n",
      "Loss:0.7100452184677124\n",
      "Loss:0.897841215133667\n",
      "Loss:0.770685613155365\n",
      "Loss:0.5792362689971924\n",
      "Loss:0.6089088916778564\n",
      "Loss:0.4863210916519165\n",
      "Loss:0.6055013537406921\n",
      "Loss:0.6344646215438843\n",
      "Loss:0.5065963864326477\n",
      "Loss:0.46111008524894714\n",
      "Loss:0.6866326928138733\n",
      "Loss:0.5472661256790161\n",
      "Loss:0.7866488695144653\n",
      "Loss:0.8768854141235352\n",
      "Loss:0.7689141631126404\n",
      "Loss:0.5805044770240784\n",
      "Loss:0.6925962567329407\n",
      "Loss:0.6098331212997437\n",
      "Loss:0.5331023335456848\n",
      "Loss:0.6836220026016235\n",
      "Loss:0.8215294480323792\n",
      "Loss:0.80368572473526\n",
      "Loss:0.584113359451294\n",
      "Loss:0.5907378196716309\n",
      "Loss:0.5917882323265076\n",
      "Loss:0.6388505697250366\n",
      "Loss:0.8019914031028748\n",
      "Loss:0.4804759919643402\n",
      "Loss:0.33214864134788513\n",
      "Loss:0.3377046287059784\n",
      "Loss:0.4913351535797119\n",
      "Loss:0.5129700899124146\n",
      "Loss:0.635035514831543\n",
      "Loss:0.6564646363258362\n",
      "Loss:0.7861446738243103\n",
      "Loss:0.6703380346298218\n",
      "Loss:0.7219785451889038\n",
      "Loss:0.514081597328186\n",
      "Loss:0.3806295394897461\n",
      "Loss:0.45786693692207336\n",
      "Loss:0.5543138980865479\n",
      "Loss:0.6778643727302551\n",
      "Loss:0.6532737016677856\n",
      "Loss:0.47491809725761414\n",
      "Loss:0.5428684949874878\n",
      "Loss:0.7613574266433716\n",
      "Loss:0.659593939781189\n",
      "Loss:0.4852123260498047\n",
      "Loss:0.6901792883872986\n",
      "Loss:0.7376331686973572\n",
      "Loss:0.6917116641998291\n",
      "Loss:0.6906830668449402\n",
      "Loss:0.7556953430175781\n",
      "Loss:0.41285428404808044\n",
      "Loss:0.4808458089828491\n",
      "Loss:0.5074493288993835\n",
      "Loss:0.4935303032398224\n",
      "Loss:0.5665197968482971\n",
      "Loss:0.5251901149749756\n",
      "Loss:0.7090660333633423\n",
      "Loss:0.6111735105514526\n",
      "Loss:0.5602843165397644\n",
      "Loss:0.545633852481842\n",
      "Loss:0.49372339248657227\n",
      "Loss:0.7012952566146851\n",
      "Loss:0.3848971426486969\n",
      "Loss:0.32159560918807983\n",
      "Loss:0.5811554193496704\n",
      "Loss:0.46391797065734863\n",
      "Loss:0.6042945981025696\n",
      "Loss:0.404837965965271\n",
      "Loss:0.3752959966659546\n",
      "Loss:0.9267938137054443\n",
      "Loss:0.7206288576126099\n",
      "Loss:0.7819652557373047\n",
      "Loss:0.3594895303249359\n",
      "Loss:0.8282905220985413\n",
      "Loss:0.8519145846366882\n",
      "Loss:0.4591549336910248\n",
      "Loss:0.5980850458145142\n",
      "Loss:0.5534780621528625\n",
      "Loss:0.5371209979057312\n",
      "Loss:0.5912230610847473\n",
      "Loss:0.9725303649902344\n",
      "Loss:0.47737807035446167\n",
      "Loss:0.6954976320266724\n",
      "Loss:0.4656452238559723\n",
      "Loss:0.455249160528183\n",
      "Loss:0.3845629096031189\n",
      "Loss:0.5367560386657715\n",
      "Loss:0.5900022983551025\n",
      "Loss:0.39029696583747864\n",
      "Loss:0.41875365376472473\n",
      "Loss:0.42022615671157837\n",
      "Loss:0.5222014784812927\n",
      "Loss:0.46593400835990906\n",
      "Loss:0.6444782018661499\n",
      "Loss:0.8185267448425293\n",
      "Loss:0.6322590112686157\n",
      "Loss:0.5066956281661987\n",
      "Loss:0.3409033417701721\n",
      "Loss:0.49171921610832214\n",
      "Loss:0.7421106696128845\n",
      "Loss:1.0047775506973267\n",
      "Loss:0.5059687495231628\n",
      "Loss:0.6509966254234314\n",
      "Loss:0.41422000527381897\n",
      "Loss:0.3963237404823303\n",
      "Loss:0.6427357792854309\n",
      "Loss:0.42682361602783203\n",
      "Loss:0.4684964418411255\n",
      "Loss:0.6648097038269043\n",
      "Loss:0.6730256676673889\n",
      "Loss:0.4123383164405823\n",
      "Loss:0.4972296357154846\n",
      "Loss:0.38423505425453186\n",
      "Loss:0.4855295419692993\n",
      "Loss:0.34469500184059143\n",
      "Loss:0.6053139567375183\n",
      "Loss:0.7258485555648804\n",
      "Loss:0.380306601524353\n",
      "Loss:0.4016439616680145\n",
      "Loss:0.4166547954082489\n",
      "Loss:0.5170087814331055\n",
      "Loss:0.41049179434776306\n",
      "Loss:0.4783386290073395\n",
      "Loss:0.3349243700504303\n",
      "Loss:0.3061423897743225\n",
      "Loss:0.3999296724796295\n",
      "Loss:0.5486968159675598\n",
      "Loss:0.36832839250564575\n",
      "Loss:0.6085433959960938\n",
      "Loss:0.5858242511749268\n",
      "Loss:0.39744317531585693\n",
      "Loss:0.3842085003852844\n",
      "Loss:0.5720070004463196\n",
      "Loss:0.38063061237335205\n",
      "Loss:0.577468752861023\n",
      "Loss:0.4259199798107147\n",
      "Loss:0.5621985197067261\n",
      "Loss:0.6859099864959717\n",
      "Loss:0.46793586015701294\n",
      "Loss:0.4630887806415558\n",
      "Loss:0.4521748125553131\n",
      "Loss:0.30913498997688293\n",
      "Loss:0.3329761326313019\n",
      "Loss:0.43482834100723267\n",
      "Loss:0.45376089215278625\n",
      "Loss:0.3725982904434204\n",
      "Loss:0.4100576937198639\n",
      "Loss:0.16967114806175232\n",
      "Loss:0.1364615261554718\n",
      "Loss:0.2847353219985962\n",
      "Loss:0.5630621314048767\n",
      "Loss:0.32857418060302734\n",
      "Loss:0.24585850536823273\n",
      "Loss:0.7627518773078918\n",
      "Loss:0.1742863953113556\n",
      "Loss:0.5653350353240967\n",
      "Epoch:0, Loss: 0.5653350353240967\n",
      "Loss:0.4574030339717865\n",
      "Loss:0.519801676273346\n",
      "Loss:0.3515865206718445\n",
      "Loss:0.4982525408267975\n",
      "Loss:0.6467849016189575\n",
      "Loss:0.5100286602973938\n",
      "Loss:0.6105766296386719\n",
      "Loss:0.5688639283180237\n",
      "Loss:0.8464309573173523\n",
      "Loss:0.3655546307563782\n",
      "Loss:0.5207856893539429\n",
      "Loss:0.4793182611465454\n",
      "Loss:0.5290214419364929\n",
      "Loss:0.3706029951572418\n",
      "Loss:0.3754388093948364\n",
      "Loss:0.42776963114738464\n",
      "Loss:0.3806871771812439\n",
      "Loss:0.4699302315711975\n",
      "Loss:0.4588819444179535\n",
      "Loss:0.3183659315109253\n",
      "Loss:0.4635468125343323\n",
      "Loss:0.48469001054763794\n",
      "Loss:0.5068405866622925\n",
      "Loss:0.533284604549408\n",
      "Loss:0.3813386559486389\n",
      "Loss:0.4198840260505676\n",
      "Loss:0.38416528701782227\n",
      "Loss:0.47186079621315\n",
      "Loss:0.4219749867916107\n",
      "Loss:0.5567904710769653\n",
      "Loss:0.37727537751197815\n",
      "Loss:0.46396124362945557\n",
      "Loss:0.5258429050445557\n",
      "Loss:0.48903271555900574\n",
      "Loss:0.4111887514591217\n",
      "Loss:0.3756652772426605\n",
      "Loss:0.48769211769104004\n",
      "Loss:0.5179304480552673\n",
      "Loss:0.49921202659606934\n",
      "Loss:0.47266843914985657\n",
      "Loss:0.6616159081459045\n",
      "Loss:0.45462045073509216\n",
      "Loss:0.34001219272613525\n",
      "Loss:0.49883148074150085\n",
      "Loss:0.4328042268753052\n",
      "Loss:0.5289206504821777\n",
      "Loss:0.32626593112945557\n",
      "Loss:0.36917048692703247\n",
      "Loss:0.39267775416374207\n",
      "Loss:0.30499663949012756\n",
      "Loss:0.5089313983917236\n",
      "Loss:0.31432104110717773\n",
      "Loss:0.4318765103816986\n",
      "Loss:0.6371269822120667\n",
      "Loss:0.5394434928894043\n",
      "Loss:0.36481353640556335\n",
      "Loss:0.6690051555633545\n",
      "Loss:0.497917503118515\n",
      "Loss:0.5133903622627258\n",
      "Loss:0.41602176427841187\n",
      "Loss:0.42583584785461426\n",
      "Loss:0.6809587478637695\n",
      "Loss:0.484031617641449\n",
      "Loss:0.4458021819591522\n",
      "Loss:0.6206291913986206\n",
      "Loss:0.42820194363594055\n",
      "Loss:0.5298492908477783\n",
      "Loss:0.607647716999054\n",
      "Loss:0.6700934171676636\n",
      "Loss:0.7323208451271057\n",
      "Loss:0.3285825848579407\n",
      "Loss:0.42197656631469727\n",
      "Loss:0.5897340178489685\n",
      "Loss:0.589164674282074\n",
      "Loss:0.407027006149292\n",
      "Loss:0.31818926334381104\n",
      "Loss:0.5920013189315796\n",
      "Loss:0.4215611517429352\n",
      "Loss:0.38229623436927795\n",
      "Loss:0.43928852677345276\n",
      "Loss:0.48910972476005554\n",
      "Loss:0.2512248754501343\n",
      "Loss:0.2825828194618225\n",
      "Loss:0.4003627300262451\n",
      "Loss:0.39397042989730835\n",
      "Loss:0.33713406324386597\n",
      "Loss:0.31704607605934143\n",
      "Loss:0.422439843416214\n",
      "Loss:0.3053503930568695\n",
      "Loss:0.3089302182197571\n",
      "Loss:0.757408857345581\n",
      "Loss:0.6405020952224731\n",
      "Loss:0.5217147469520569\n",
      "Loss:0.3762243688106537\n",
      "Loss:0.34624162316322327\n",
      "Loss:0.5062709450721741\n",
      "Loss:0.5233845114707947\n",
      "Loss:0.5420333743095398\n",
      "Loss:0.6837558150291443\n",
      "Loss:0.5861386060714722\n",
      "Loss:0.35837322473526\n",
      "Loss:0.7243540287017822\n",
      "Loss:0.5832046866416931\n",
      "Loss:0.37606313824653625\n",
      "Loss:0.36609986424446106\n",
      "Loss:0.35746198892593384\n",
      "Loss:0.42581045627593994\n",
      "Loss:0.4787031412124634\n",
      "Loss:0.46183016896247864\n",
      "Loss:0.7284295558929443\n",
      "Loss:0.6097449064254761\n",
      "Loss:0.6332493424415588\n",
      "Loss:0.5647604465484619\n",
      "Loss:0.5566654205322266\n",
      "Loss:0.6924347281455994\n",
      "Loss:0.7325791716575623\n",
      "Loss:0.4731270372867584\n",
      "Loss:0.3722235858440399\n",
      "Loss:0.5858439803123474\n",
      "Loss:0.4075992703437805\n",
      "Loss:0.326884925365448\n",
      "Loss:0.4812571406364441\n",
      "Loss:0.4286516308784485\n",
      "Loss:0.6446443796157837\n",
      "Loss:0.4814226031303406\n",
      "Loss:0.5859799385070801\n",
      "Loss:0.42978790402412415\n",
      "Loss:0.2412029206752777\n",
      "Loss:0.35526591539382935\n",
      "Loss:0.31334763765335083\n",
      "Loss:0.5637363791465759\n",
      "Loss:0.5130883455276489\n",
      "Loss:0.48669278621673584\n",
      "Loss:0.4101785123348236\n",
      "Loss:0.5711360573768616\n",
      "Loss:0.37760457396507263\n",
      "Loss:0.4376653730869293\n",
      "Loss:0.6901385188102722\n",
      "Loss:0.5701647400856018\n",
      "Loss:0.4141579866409302\n",
      "Loss:0.47438374161720276\n",
      "Loss:0.3132465183734894\n",
      "Loss:0.23112809658050537\n",
      "Loss:0.4977107644081116\n",
      "Loss:0.37952736020088196\n",
      "Loss:0.46655505895614624\n",
      "Loss:0.41601210832595825\n",
      "Loss:0.33362317085266113\n",
      "Loss:0.2852258086204529\n",
      "Loss:0.4777974784374237\n",
      "Loss:0.4715079963207245\n",
      "Loss:0.44381004571914673\n",
      "Loss:0.3876805007457733\n",
      "Loss:0.23241117596626282\n",
      "Loss:0.3923950791358948\n",
      "Loss:0.44775164127349854\n",
      "Loss:0.46434032917022705\n",
      "Loss:0.48171335458755493\n",
      "Loss:0.3746032416820526\n",
      "Loss:0.23138339817523956\n",
      "Loss:0.3589074909687042\n",
      "Loss:0.49814799427986145\n",
      "Loss:0.575086772441864\n",
      "Loss:0.6034654974937439\n",
      "Loss:0.4733760356903076\n",
      "Loss:0.3657269775867462\n",
      "Loss:0.3209017515182495\n",
      "Loss:0.4211151599884033\n",
      "Loss:0.4403589069843292\n",
      "Loss:0.40184903144836426\n",
      "Loss:0.20113973319530487\n",
      "Loss:0.32074376940727234\n",
      "Loss:0.4020366072654724\n",
      "Loss:0.5286664962768555\n",
      "Loss:0.36432144045829773\n",
      "Loss:0.5635953545570374\n",
      "Loss:0.6606077551841736\n",
      "Loss:0.3887665271759033\n",
      "Loss:0.38909006118774414\n",
      "Loss:0.2890903651714325\n",
      "Loss:0.41920745372772217\n",
      "Loss:0.31931766867637634\n",
      "Loss:0.3624105453491211\n",
      "Loss:0.36670982837677\n",
      "Loss:0.46036529541015625\n",
      "Loss:0.47213083505630493\n",
      "Loss:0.4338599443435669\n",
      "Loss:0.402814120054245\n",
      "Loss:0.26177966594696045\n",
      "Loss:0.4845258593559265\n",
      "Loss:0.3142394721508026\n",
      "Loss:0.5370665192604065\n",
      "Loss:0.5154367089271545\n",
      "Loss:0.4906957745552063\n",
      "Loss:0.5050980448722839\n",
      "Loss:0.35715773701667786\n",
      "Loss:0.44164741039276123\n",
      "Loss:0.5562328100204468\n",
      "Loss:0.21290802955627441\n",
      "Loss:0.38280147314071655\n",
      "Loss:0.2668914198875427\n",
      "Loss:0.5236836671829224\n",
      "Loss:0.41120532155036926\n",
      "Loss:0.3343237638473511\n",
      "Loss:0.3653758466243744\n",
      "Loss:0.3893597722053528\n",
      "Loss:0.6205509305000305\n",
      "Loss:0.5118566751480103\n",
      "Loss:0.6607396602630615\n",
      "Loss:0.3792494237422943\n",
      "Loss:0.3536570072174072\n",
      "Loss:0.27519065141677856\n",
      "Loss:0.6786314249038696\n",
      "Loss:0.37164586782455444\n",
      "Loss:0.5179852247238159\n",
      "Loss:0.32741430401802063\n",
      "Loss:0.3889857232570648\n",
      "Loss:0.4311797320842743\n",
      "Loss:0.3188852369785309\n",
      "Loss:0.3351355195045471\n",
      "Loss:0.45853012800216675\n",
      "Loss:0.5184957981109619\n",
      "Loss:0.48761022090911865\n",
      "Loss:0.617925763130188\n",
      "Loss:0.34592676162719727\n",
      "Loss:0.32372432947158813\n",
      "Loss:0.3657192587852478\n",
      "Loss:0.5962235927581787\n",
      "Loss:0.3476848006248474\n",
      "Loss:0.4278404414653778\n",
      "Loss:0.303295761346817\n",
      "Loss:0.46068069338798523\n",
      "Loss:0.4898724853992462\n",
      "Loss:0.5981444716453552\n",
      "Loss:0.5467545986175537\n",
      "Loss:0.6001569628715515\n",
      "Loss:0.36750638484954834\n",
      "Loss:0.32160162925720215\n",
      "Loss:0.47784802317619324\n",
      "Loss:0.564202606678009\n",
      "Loss:0.46034055948257446\n",
      "Loss:0.5662992596626282\n",
      "Loss:0.44703730940818787\n",
      "Loss:0.5680413246154785\n",
      "Loss:0.6830921769142151\n",
      "Loss:0.5132134556770325\n",
      "Loss:0.4601832926273346\n",
      "Loss:0.76454758644104\n",
      "Loss:0.4126025140285492\n",
      "Loss:0.4698607325553894\n",
      "Loss:0.4929414391517639\n",
      "Loss:0.5430234670639038\n",
      "Loss:0.4953359365463257\n",
      "Loss:0.6173672676086426\n",
      "Loss:0.24412094056606293\n",
      "Loss:0.45118096470832825\n",
      "Loss:0.48910489678382874\n",
      "Loss:0.3046413064002991\n",
      "Loss:0.4933033883571625\n",
      "Loss:0.33586597442626953\n",
      "Loss:0.5205479860305786\n",
      "Loss:0.5013739466667175\n",
      "Loss:0.36454325914382935\n",
      "Loss:0.3192828595638275\n",
      "Loss:0.16046299040317535\n",
      "Loss:0.24389879405498505\n",
      "Loss:0.3387637138366699\n",
      "Loss:0.24962027370929718\n",
      "Loss:0.43624943494796753\n",
      "Loss:0.5088284611701965\n",
      "Loss:0.44293922185897827\n",
      "Loss:0.4405185580253601\n",
      "Loss:0.4987027049064636\n",
      "Loss:0.26199889183044434\n",
      "Loss:0.3478914797306061\n",
      "Loss:0.41118142008781433\n",
      "Loss:0.31447890400886536\n",
      "Loss:0.367959588766098\n",
      "Loss:0.3980468809604645\n",
      "Loss:0.3236061930656433\n",
      "Loss:0.4063957631587982\n",
      "Loss:0.4824356436729431\n",
      "Loss:0.3942354619503021\n",
      "Loss:0.23646694421768188\n",
      "Loss:0.5272694230079651\n",
      "Loss:0.2702781558036804\n",
      "Loss:0.2377222180366516\n",
      "Loss:0.4363889694213867\n",
      "Loss:0.31890541315078735\n",
      "Loss:0.41666772961616516\n",
      "Loss:0.4333786964416504\n",
      "Loss:0.5864399671554565\n",
      "Loss:0.837783932685852\n",
      "Loss:0.4571446478366852\n",
      "Loss:0.5280778408050537\n",
      "Loss:0.4631763994693756\n",
      "Loss:0.38943082094192505\n",
      "Loss:0.2950344681739807\n",
      "Loss:0.4007539451122284\n",
      "Loss:0.4552096426486969\n",
      "Loss:0.4556828737258911\n",
      "Loss:0.44073405861854553\n",
      "Loss:0.3528844118118286\n",
      "Loss:0.22411474585533142\n",
      "Loss:0.3379665017127991\n",
      "Loss:0.27271848917007446\n",
      "Loss:0.3390823304653168\n",
      "Loss:0.7482727766036987\n",
      "Loss:0.45833203196525574\n",
      "Loss:0.3674975335597992\n",
      "Loss:0.3838048279285431\n",
      "Loss:0.6002121567726135\n",
      "Loss:0.5177383422851562\n",
      "Loss:0.34532877802848816\n",
      "Loss:0.41752445697784424\n",
      "Loss:0.29939767718315125\n",
      "Loss:0.36367011070251465\n",
      "Loss:0.41419920325279236\n",
      "Loss:0.31235358119010925\n",
      "Loss:0.26703596115112305\n",
      "Loss:0.49483147263526917\n",
      "Loss:0.3082158863544464\n",
      "Loss:0.5970125794410706\n",
      "Loss:0.6615098118782043\n",
      "Loss:0.48676013946533203\n",
      "Loss:0.3471127450466156\n",
      "Loss:0.4692946672439575\n",
      "Loss:0.4118032157421112\n",
      "Loss:0.37679266929626465\n",
      "Loss:0.4028548300266266\n",
      "Loss:0.5257084965705872\n",
      "Loss:0.5721365809440613\n",
      "Loss:0.41852644085884094\n",
      "Loss:0.4166897237300873\n",
      "Loss:0.45262131094932556\n",
      "Loss:0.47806060314178467\n",
      "Loss:0.5205724239349365\n",
      "Loss:0.3043597638607025\n",
      "Loss:0.19049912691116333\n",
      "Loss:0.2017565369606018\n",
      "Loss:0.35266125202178955\n",
      "Loss:0.3269168734550476\n",
      "Loss:0.43427497148513794\n",
      "Loss:0.4357323944568634\n",
      "Loss:0.5093468427658081\n",
      "Loss:0.42870649695396423\n",
      "Loss:0.5089539289474487\n",
      "Loss:0.37328341603279114\n",
      "Loss:0.22418531775474548\n",
      "Loss:0.29029595851898193\n",
      "Loss:0.3760043978691101\n",
      "Loss:0.5234304070472717\n",
      "Loss:0.44397521018981934\n",
      "Loss:0.31843242049217224\n",
      "Loss:0.3331306576728821\n",
      "Loss:0.5769564509391785\n",
      "Loss:0.4628831744194031\n",
      "Loss:0.3570513129234314\n",
      "Loss:0.5067009925842285\n",
      "Loss:0.5426564812660217\n",
      "Loss:0.47907713055610657\n",
      "Loss:0.5212595462799072\n",
      "Loss:0.5676192045211792\n",
      "Loss:0.2468724101781845\n",
      "Loss:0.28337565064430237\n",
      "Loss:0.35950443148612976\n",
      "Loss:0.3279705345630646\n",
      "Loss:0.36951664090156555\n",
      "Loss:0.34508126974105835\n",
      "Loss:0.4937077462673187\n",
      "Loss:0.41410186886787415\n",
      "Loss:0.3750351071357727\n",
      "Loss:0.3962506949901581\n",
      "Loss:0.32201412320137024\n",
      "Loss:0.5466375946998596\n",
      "Loss:0.23902086913585663\n",
      "Loss:0.20470471680164337\n",
      "Loss:0.47120749950408936\n",
      "Loss:0.3132466673851013\n",
      "Loss:0.39829131960868835\n",
      "Loss:0.23934544622898102\n",
      "Loss:0.23502543568611145\n",
      "Loss:0.7630252838134766\n",
      "Loss:0.5268656611442566\n",
      "Loss:0.5342406630516052\n",
      "Loss:0.19281020760536194\n",
      "Loss:0.6218231916427612\n",
      "Loss:0.6824682354927063\n",
      "Loss:0.3277568817138672\n",
      "Loss:0.43083199858665466\n",
      "Loss:0.3737923204898834\n",
      "Loss:0.3871336579322815\n",
      "Loss:0.4577365815639496\n",
      "Loss:0.7960633635520935\n",
      "Loss:0.30967602133750916\n",
      "Loss:0.5138425230979919\n",
      "Loss:0.29947543144226074\n",
      "Loss:0.2851804494857788\n",
      "Loss:0.2299966812133789\n",
      "Loss:0.354325532913208\n",
      "Loss:0.4264964461326599\n",
      "Loss:0.2592138648033142\n",
      "Loss:0.2935135066509247\n",
      "Loss:0.2801455557346344\n",
      "Loss:0.35492730140686035\n",
      "Loss:0.33133161067962646\n",
      "Loss:0.5055174827575684\n",
      "Loss:0.5922573208808899\n",
      "Loss:0.4692005217075348\n",
      "Loss:0.3918394148349762\n",
      "Loss:0.2164948582649231\n",
      "Loss:0.3900669813156128\n",
      "Loss:0.5603400468826294\n",
      "Loss:0.8423532247543335\n",
      "Loss:0.3559301197528839\n",
      "Loss:0.4927622973918915\n",
      "Loss:0.25790494680404663\n",
      "Loss:0.2353483885526657\n",
      "Loss:0.4583154022693634\n",
      "Loss:0.3006523549556732\n",
      "Loss:0.3353787660598755\n",
      "Loss:0.5159235000610352\n",
      "Loss:0.45692160725593567\n",
      "Loss:0.29441994428634644\n",
      "Loss:0.36386221647262573\n",
      "Loss:0.24480217695236206\n",
      "Loss:0.35787340998649597\n",
      "Loss:0.2377982884645462\n",
      "Loss:0.46689972281455994\n",
      "Loss:0.5524165034294128\n",
      "Loss:0.2558940351009369\n",
      "Loss:0.31485632061958313\n",
      "Loss:0.33334848284721375\n",
      "Loss:0.3474050462245941\n",
      "Loss:0.28088071942329407\n",
      "Loss:0.3891768753528595\n",
      "Loss:0.23112857341766357\n",
      "Loss:0.20395854115486145\n",
      "Loss:0.2431292086839676\n",
      "Loss:0.41994673013687134\n",
      "Loss:0.24673326313495636\n",
      "Loss:0.4696558117866516\n",
      "Loss:0.4343905746936798\n",
      "Loss:0.2637070119380951\n",
      "Loss:0.2564227283000946\n",
      "Loss:0.40149956941604614\n",
      "Loss:0.2276497781276703\n",
      "Loss:0.3944894075393677\n",
      "Loss:0.2580156922340393\n",
      "Loss:0.36887237429618835\n",
      "Loss:0.5063408613204956\n",
      "Loss:0.3323357105255127\n",
      "Loss:0.32275232672691345\n",
      "Loss:0.3552711606025696\n",
      "Loss:0.20919162034988403\n",
      "Loss:0.23673485219478607\n",
      "Loss:0.3066518008708954\n",
      "Loss:0.2964174747467041\n",
      "Loss:0.22542297840118408\n",
      "Loss:0.2883095443248749\n",
      "Loss:0.09900318831205368\n",
      "Loss:0.061855122447013855\n",
      "Loss:0.19550065696239471\n",
      "Loss:0.4249860942363739\n",
      "Loss:0.2293599247932434\n",
      "Loss:0.13237932324409485\n",
      "Loss:0.6550478935241699\n",
      "Loss:0.10297829657793045\n",
      "Loss:0.422017365694046\n",
      "Loss:0.34881120920181274\n",
      "Loss:0.37828877568244934\n",
      "Loss:0.25244662165641785\n",
      "Loss:0.395783007144928\n",
      "Loss:0.49872642755508423\n",
      "Loss:0.3597423732280731\n",
      "Loss:0.4716213047504425\n",
      "Loss:0.4261445999145508\n",
      "Loss:0.6953111290931702\n",
      "Loss:0.27706247568130493\n",
      "Loss:0.4549432694911957\n",
      "Loss:0.34340715408325195\n",
      "Loss:0.38546326756477356\n",
      "Loss:0.2723637521266937\n",
      "Loss:0.258482426404953\n",
      "Loss:0.2963896691799164\n",
      "Loss:0.28527316451072693\n",
      "Loss:0.3571801781654358\n",
      "Loss:0.35227030515670776\n",
      "Loss:0.1999448537826538\n",
      "Loss:0.36017969250679016\n",
      "Loss:0.36786073446273804\n",
      "Loss:0.3732110857963562\n",
      "Loss:0.419860303401947\n",
      "Loss:0.2508709132671356\n",
      "Loss:0.3246332108974457\n",
      "Loss:0.2788326144218445\n",
      "Loss:0.3362058103084564\n",
      "Loss:0.347659707069397\n",
      "Loss:0.4163607954978943\n",
      "Loss:0.24241673946380615\n",
      "Loss:0.35333317518234253\n",
      "Loss:0.4139055609703064\n",
      "Loss:0.3653155267238617\n",
      "Loss:0.27364474534988403\n",
      "Loss:0.2672352194786072\n",
      "Loss:0.35293033719062805\n",
      "Loss:0.4093400835990906\n",
      "Loss:0.3969338536262512\n",
      "Loss:0.3451591730117798\n",
      "Loss:0.521457850933075\n",
      "Loss:0.3545738160610199\n",
      "Loss:0.2438724786043167\n",
      "Loss:0.3665125072002411\n",
      "Loss:0.34410449862480164\n",
      "Loss:0.42312395572662354\n",
      "Loss:0.20782704651355743\n",
      "Loss:0.30375418066978455\n",
      "Loss:0.2653176784515381\n",
      "Loss:0.2052401453256607\n",
      "Loss:0.40488162636756897\n",
      "Loss:0.21212317049503326\n",
      "Loss:0.3586142957210541\n",
      "Loss:0.5149518847465515\n",
      "Loss:0.40395691990852356\n",
      "Loss:0.2659316658973694\n",
      "Loss:0.520900547504425\n",
      "Loss:0.3632088005542755\n",
      "Loss:0.3739529550075531\n",
      "Loss:0.2996811866760254\n",
      "Loss:0.30063596367836\n",
      "Loss:0.5366112589836121\n",
      "Loss:0.3377710282802582\n",
      "Loss:0.3455989360809326\n",
      "Loss:0.5119988918304443\n",
      "Loss:0.2984890639781952\n",
      "Loss:0.43583759665489197\n",
      "Loss:0.4698716700077057\n",
      "Loss:0.5990787148475647\n",
      "Loss:0.6143754124641418\n",
      "Loss:0.2313300520181656\n",
      "Loss:0.3319035768508911\n",
      "Loss:0.4666275680065155\n",
      "Loss:0.4896279275417328\n",
      "Loss:0.31684502959251404\n",
      "Loss:0.2311834692955017\n",
      "Loss:0.45498260855674744\n",
      "Loss:0.3313794434070587\n",
      "Loss:0.29222190380096436\n",
      "Loss:0.35683703422546387\n",
      "Loss:0.3635197877883911\n",
      "Loss:0.1790451854467392\n",
      "Loss:0.17461690306663513\n",
      "Loss:0.29921939969062805\n",
      "Loss:0.31388548016548157\n",
      "Loss:0.23614844679832458\n",
      "Loss:0.22531947493553162\n",
      "Loss:0.3176785707473755\n",
      "Loss:0.2003324329853058\n",
      "Loss:0.2297976166009903\n",
      "Loss:0.6243269443511963\n",
      "Loss:0.5058987736701965\n",
      "Loss:0.417061448097229\n",
      "Loss:0.2723959684371948\n",
      "Loss:0.2402324378490448\n",
      "Loss:0.3870972692966461\n",
      "Loss:0.40097588300704956\n",
      "Loss:0.39880621433258057\n",
      "Loss:0.5818012952804565\n",
      "Loss:0.46556156873703003\n",
      "Loss:0.26666074991226196\n",
      "Loss:0.5742890238761902\n",
      "Loss:0.45386531949043274\n",
      "Loss:0.2641447186470032\n",
      "Loss:0.26195263862609863\n",
      "Loss:0.26300689578056335\n",
      "Loss:0.32853493094444275\n",
      "Loss:0.379037082195282\n",
      "Loss:0.34979113936424255\n",
      "Loss:0.5803713798522949\n",
      "Loss:0.48850905895233154\n",
      "Loss:0.4964674115180969\n",
      "Loss:0.4481165111064911\n",
      "Loss:0.4270000755786896\n",
      "Loss:0.5309339165687561\n",
      "Loss:0.6071101427078247\n",
      "Loss:0.36437150835990906\n",
      "Loss:0.2825015187263489\n",
      "Loss:0.484016478061676\n",
      "Loss:0.3221500515937805\n",
      "Loss:0.2536061108112335\n",
      "Loss:0.38475659489631653\n",
      "Loss:0.3304101526737213\n",
      "Loss:0.48712897300720215\n",
      "Loss:0.3711077570915222\n",
      "Loss:0.45943576097488403\n",
      "Loss:0.2989236116409302\n",
      "Loss:0.1909949630498886\n",
      "Loss:0.2751212418079376\n",
      "Loss:0.2511650025844574\n",
      "Loss:0.4568749666213989\n",
      "Loss:0.406032532453537\n",
      "Loss:0.3558901846408844\n",
      "Loss:0.29081615805625916\n",
      "Loss:0.4409068524837494\n",
      "Loss:0.26725366711616516\n",
      "Loss:0.3382677733898163\n",
      "Loss:0.5764263868331909\n",
      "Loss:0.42448362708091736\n",
      "Loss:0.34294068813323975\n",
      "Loss:0.3979857563972473\n",
      "Loss:0.22823910415172577\n",
      "Loss:0.16395732760429382\n",
      "Loss:0.36838722229003906\n",
      "Loss:0.2863028049468994\n",
      "Loss:0.3686427175998688\n",
      "Loss:0.34291601181030273\n",
      "Loss:0.26589491963386536\n",
      "Loss:0.19707690179347992\n",
      "Loss:0.34544551372528076\n",
      "Loss:0.3543718457221985\n",
      "Loss:0.34469300508499146\n",
      "Loss:0.32237425446510315\n",
      "Loss:0.179451584815979\n",
      "Loss:0.30929526686668396\n",
      "Loss:0.3554314076900482\n",
      "Loss:0.36457550525665283\n",
      "Loss:0.4107162058353424\n",
      "Loss:0.29638877511024475\n",
      "Loss:0.13914749026298523\n",
      "Loss:0.2964118719100952\n",
      "Loss:0.44885268807411194\n",
      "Loss:0.4607612192630768\n",
      "Loss:0.5471611618995667\n",
      "Loss:0.4069942533969879\n",
      "Loss:0.32310283184051514\n",
      "Loss:0.24998846650123596\n",
      "Loss:0.3158728778362274\n",
      "Loss:0.3487871289253235\n",
      "Loss:0.3177318871021271\n",
      "Loss:0.12433620542287827\n",
      "Loss:0.2737559676170349\n",
      "Loss:0.28661707043647766\n",
      "Loss:0.41652825474739075\n",
      "Loss:0.2730872333049774\n",
      "Loss:0.4358786940574646\n",
      "Loss:0.5818753242492676\n",
      "Loss:0.29544854164123535\n",
      "Loss:0.2782343626022339\n",
      "Loss:0.21985886991024017\n",
      "Loss:0.32737451791763306\n",
      "Loss:0.23772677779197693\n",
      "Loss:0.28188955783843994\n",
      "Loss:0.28576815128326416\n",
      "Loss:0.37746715545654297\n",
      "Loss:0.3651620149612427\n",
      "Loss:0.3215264678001404\n",
      "Loss:0.33819857239723206\n",
      "Loss:0.19301170110702515\n",
      "Loss:0.3777349293231964\n",
      "Loss:0.2285836637020111\n",
      "Loss:0.44080981612205505\n",
      "Loss:0.3966846764087677\n",
      "Loss:0.40132972598075867\n",
      "Loss:0.40494540333747864\n",
      "Loss:0.2597384452819824\n",
      "Loss:0.33859097957611084\n",
      "Loss:0.45328113436698914\n",
      "Loss:0.14483091235160828\n",
      "Loss:0.3313741385936737\n",
      "Loss:0.21890789270401\n",
      "Loss:0.4420170485973358\n",
      "Loss:0.3249306082725525\n",
      "Loss:0.2549964189529419\n",
      "Loss:0.2962155044078827\n",
      "Loss:0.3151986002922058\n",
      "Loss:0.5077126622200012\n",
      "Loss:0.434287965297699\n",
      "Loss:0.5903725624084473\n",
      "Loss:0.33187031745910645\n",
      "Loss:0.2798435091972351\n",
      "Loss:0.20433810353279114\n",
      "Loss:0.6078528165817261\n",
      "Loss:0.27463313937187195\n",
      "Loss:0.41713911294937134\n",
      "Loss:0.26055407524108887\n",
      "Loss:0.32072022557258606\n",
      "Loss:0.3485899865627289\n",
      "Loss:0.2564126253128052\n",
      "Loss:0.2676243782043457\n",
      "Loss:0.3615107536315918\n",
      "Loss:0.45689740777015686\n",
      "Loss:0.41575387120246887\n",
      "Loss:0.5555819869041443\n",
      "Loss:0.2626326084136963\n",
      "Loss:0.23180335760116577\n",
      "Loss:0.2881532907485962\n",
      "Loss:0.4732974171638489\n",
      "Loss:0.2903580069541931\n",
      "Loss:0.37063297629356384\n",
      "Loss:0.21880924701690674\n",
      "Loss:0.36039528250694275\n",
      "Loss:0.37893784046173096\n",
      "Loss:0.5182904005050659\n",
      "Loss:0.4564146399497986\n",
      "Loss:0.4774397611618042\n",
      "Loss:0.29597485065460205\n",
      "Loss:0.24955032765865326\n",
      "Loss:0.35870447754859924\n",
      "Loss:0.45932504534721375\n",
      "Loss:0.3726726770401001\n",
      "Loss:0.45559564232826233\n",
      "Loss:0.3621540367603302\n",
      "Loss:0.4872039258480072\n",
      "Loss:0.5851960182189941\n",
      "Loss:0.41053882241249084\n",
      "Loss:0.37727129459381104\n",
      "Loss:0.6439721584320068\n",
      "Loss:0.3211349546909332\n",
      "Loss:0.3600238263607025\n",
      "Loss:0.44300249218940735\n",
      "Loss:0.4256298243999481\n",
      "Loss:0.397675096988678\n",
      "Loss:0.5105741620063782\n",
      "Loss:0.16943581402301788\n",
      "Loss:0.3904663324356079\n",
      "Loss:0.4130273461341858\n",
      "Loss:0.2520933747291565\n",
      "Loss:0.38465186953544617\n",
      "Loss:0.25331786274909973\n",
      "Loss:0.4247705936431885\n",
      "Loss:0.386086106300354\n",
      "Loss:0.2740851640701294\n",
      "Loss:0.2612840235233307\n",
      "Loss:0.11620136350393295\n",
      "Loss:0.19832681119441986\n",
      "Loss:0.2568615972995758\n",
      "Loss:0.18946179747581482\n",
      "Loss:0.3652903735637665\n",
      "Loss:0.42487606406211853\n",
      "Loss:0.33998796343803406\n",
      "Loss:0.34346282482147217\n",
      "Loss:0.40604740381240845\n",
      "Loss:0.20137952268123627\n",
      "Loss:0.2648659646511078\n",
      "Loss:0.34111496806144714\n",
      "Loss:0.23357123136520386\n",
      "Loss:0.31638744473457336\n",
      "Loss:0.3094232380390167\n",
      "Loss:0.2468477040529251\n",
      "Loss:0.31819039583206177\n",
      "Loss:0.3767802119255066\n",
      "Loss:0.3273729085922241\n",
      "Loss:0.19115959107875824\n",
      "Loss:0.44283950328826904\n",
      "Loss:0.21881258487701416\n",
      "Loss:0.18538117408752441\n",
      "Loss:0.35846319794654846\n",
      "Loss:0.2457440346479416\n",
      "Loss:0.34181472659111023\n",
      "Loss:0.36503785848617554\n",
      "Loss:0.5323158502578735\n",
      "Loss:0.7483548521995544\n",
      "Loss:0.36454224586486816\n",
      "Loss:0.4540134370326996\n",
      "Loss:0.37999317049980164\n",
      "Loss:0.3117254078388214\n",
      "Loss:0.22706347703933716\n",
      "Loss:0.33079656958580017\n",
      "Loss:0.3727979063987732\n",
      "Loss:0.37510237097740173\n",
      "Loss:0.36420395970344543\n",
      "Loss:0.2849987745285034\n",
      "Loss:0.16487102210521698\n",
      "Loss:0.2589317262172699\n",
      "Loss:0.22945468127727509\n",
      "Loss:0.2821517586708069\n",
      "Loss:0.6626565456390381\n",
      "Loss:0.3793381154537201\n",
      "Loss:0.30815815925598145\n",
      "Loss:0.27942323684692383\n",
      "Loss:0.47109970450401306\n",
      "Loss:0.4111036956310272\n",
      "Loss:0.26965850591659546\n",
      "Loss:0.3543514013290405\n",
      "Loss:0.23371978104114532\n",
      "Loss:0.2877558469772339\n",
      "Loss:0.3433429002761841\n",
      "Loss:0.2564328610897064\n",
      "Loss:0.19777239859104156\n",
      "Loss:0.42082229256629944\n",
      "Loss:0.24189037084579468\n",
      "Loss:0.5251331925392151\n",
      "Loss:0.5810461044311523\n",
      "Loss:0.3910421133041382\n",
      "Loss:0.26979222893714905\n",
      "Loss:0.3911828398704529\n",
      "Loss:0.3346009850502014\n",
      "Loss:0.3197050094604492\n",
      "Loss:0.3108593821525574\n",
      "Loss:0.415016770362854\n",
      "Loss:0.4951110780239105\n",
      "Loss:0.35392242670059204\n",
      "Loss:0.35334134101867676\n",
      "Loss:0.39296308159828186\n",
      "Loss:0.4206881821155548\n",
      "Loss:0.42101818323135376\n",
      "Loss:0.2523624002933502\n",
      "Loss:0.1471261829137802\n",
      "Loss:0.16314882040023804\n",
      "Loss:0.3127373456954956\n",
      "Loss:0.266268789768219\n",
      "Loss:0.36015182733535767\n",
      "Loss:0.3717484176158905\n",
      "Loss:0.4016420543193817\n",
      "Loss:0.32952451705932617\n",
      "Loss:0.41712838411331177\n",
      "Loss:0.3272705078125\n",
      "Loss:0.1646701544523239\n",
      "Loss:0.24816294014453888\n",
      "Loss:0.31788796186447144\n",
      "Loss:0.4507516920566559\n",
      "Loss:0.3630114197731018\n",
      "Loss:0.2804781496524811\n",
      "Loss:0.25702187418937683\n",
      "Loss:0.5004111528396606\n",
      "Loss:0.38675177097320557\n",
      "Loss:0.30981168150901794\n",
      "Loss:0.4383965730667114\n",
      "Loss:0.46751126646995544\n",
      "Loss:0.39982667565345764\n",
      "Loss:0.4495609402656555\n",
      "Loss:0.48398956656455994\n",
      "Loss:0.18330565094947815\n",
      "Loss:0.2129950225353241\n",
      "Loss:0.3059861660003662\n",
      "Loss:0.2621297836303711\n",
      "Loss:0.29046374559402466\n",
      "Loss:0.2829655408859253\n",
      "Loss:0.4190378189086914\n",
      "Loss:0.3391018211841583\n",
      "Loss:0.30761823058128357\n",
      "Loss:0.33431485295295715\n",
      "Loss:0.25288572907447815\n",
      "Loss:0.469534695148468\n",
      "Loss:0.19488364458084106\n",
      "Loss:0.16282257437705994\n",
      "Loss:0.42146849632263184\n",
      "Loss:0.24920248985290527\n",
      "Loss:0.3104347586631775\n",
      "Loss:0.18360285460948944\n",
      "Loss:0.19224919378757477\n",
      "Loss:0.6993969678878784\n",
      "Loss:0.4415702223777771\n",
      "Loss:0.4330730140209198\n",
      "Loss:0.14105306565761566\n",
      "Loss:0.5295225977897644\n",
      "Loss:0.6052556037902832\n",
      "Loss:0.27767154574394226\n",
      "Loss:0.38024604320526123\n",
      "Loss:0.3094906806945801\n",
      "Loss:0.31861984729766846\n",
      "Loss:0.40245649218559265\n",
      "Loss:0.6936502456665039\n",
      "Loss:0.25329869985580444\n",
      "Loss:0.42877253890037537\n",
      "Loss:0.24962937831878662\n",
      "Loss:0.23452867567539215\n",
      "Loss:0.17144274711608887\n",
      "Loss:0.28222912549972534\n",
      "Loss:0.3617231249809265\n",
      "Loss:0.20320463180541992\n",
      "Loss:0.2474195808172226\n",
      "Loss:0.2282601296901703\n",
      "Loss:0.30681315064430237\n",
      "Loss:0.2788337469100952\n",
      "Loss:0.4431252181529999\n",
      "Loss:0.4901675283908844\n",
      "Loss:0.3950493335723877\n",
      "Loss:0.34479179978370667\n",
      "Loss:0.16936348378658295\n",
      "Loss:0.34012895822525024\n",
      "Loss:0.47172582149505615\n",
      "Loss:0.7395572662353516\n",
      "Loss:0.2872341573238373\n",
      "Loss:0.4187832176685333\n",
      "Loss:0.18969716131687164\n",
      "Loss:0.17973333597183228\n",
      "Loss:0.39058610796928406\n",
      "Loss:0.25707218050956726\n",
      "Loss:0.29401278495788574\n",
      "Loss:0.45639893412590027\n",
      "Loss:0.3617739677429199\n",
      "Loss:0.2383388876914978\n",
      "Loss:0.3065885007381439\n",
      "Loss:0.19620564579963684\n",
      "Loss:0.3110594153404236\n",
      "Loss:0.19537486135959625\n",
      "Loss:0.39405587315559387\n",
      "Loss:0.45927339792251587\n",
      "Loss:0.21008943021297455\n",
      "Loss:0.2885267436504364\n",
      "Loss:0.29339057207107544\n",
      "Loss:0.2794128656387329\n",
      "Loss:0.22699743509292603\n",
      "Loss:0.35428133606910706\n",
      "Loss:0.19814273715019226\n",
      "Loss:0.17030827701091766\n",
      "Loss:0.1836346834897995\n",
      "Loss:0.36193105578422546\n",
      "Loss:0.20233634114265442\n",
      "Loss:0.4072816073894501\n",
      "Loss:0.3640105724334717\n",
      "Loss:0.21092450618743896\n",
      "Loss:0.20710434019565582\n",
      "Loss:0.3309515416622162\n",
      "Loss:0.17010992765426636\n",
      "Loss:0.32657939195632935\n",
      "Loss:0.18767309188842773\n",
      "Loss:0.2948109805583954\n",
      "Loss:0.4357071816921234\n",
      "Loss:0.2725207507610321\n",
      "Loss:0.2603265941143036\n",
      "Loss:0.31197312474250793\n",
      "Loss:0.1658417284488678\n",
      "Loss:0.19859130680561066\n",
      "Loss:0.24925629794597626\n",
      "Loss:0.22809989750385284\n",
      "Loss:0.17532028257846832\n",
      "Loss:0.23885077238082886\n",
      "Loss:0.07511317729949951\n",
      "Loss:0.04249945655465126\n",
      "Loss:0.15898136794567108\n",
      "Loss:0.35296982526779175\n",
      "Loss:0.18944396078586578\n",
      "Loss:0.09304573386907578\n",
      "Loss:0.6106955409049988\n",
      "Loss:0.07635121792554855\n",
      "Loss:0.3684803545475006\n",
      "Loss:0.2907947897911072\n",
      "Loss:0.3280515670776367\n",
      "Loss:0.2065637856721878\n",
      "Loss:0.35198816657066345\n",
      "Loss:0.4314635992050171\n",
      "Loss:0.2906511127948761\n",
      "Loss:0.39988142251968384\n",
      "Loss:0.36015382409095764\n",
      "Loss:0.6273896098136902\n",
      "Loss:0.24479719996452332\n",
      "Loss:0.42310160398483276\n",
      "Loss:0.27917107939720154\n",
      "Loss:0.32116249203681946\n",
      "Loss:0.23327672481536865\n",
      "Loss:0.20538796484470367\n",
      "Loss:0.24389080703258514\n",
      "Loss:0.2440863698720932\n",
      "Loss:0.308258056640625\n",
      "Loss:0.30595001578330994\n",
      "Loss:0.1532377302646637\n",
      "Loss:0.31415724754333496\n",
      "Loss:0.30955979228019714\n",
      "Loss:0.310295432806015\n",
      "Loss:0.36070212721824646\n",
      "Loss:0.20169448852539062\n",
      "Loss:0.2846969664096832\n",
      "Loss:0.22951911389827728\n",
      "Loss:0.2799568474292755\n",
      "Loss:0.31362009048461914\n",
      "Loss:0.3423061966896057\n",
      "Loss:0.18265311419963837\n",
      "Loss:0.3061966300010681\n",
      "Loss:0.34532928466796875\n",
      "Loss:0.313859224319458\n",
      "Loss:0.21697860956192017\n",
      "Loss:0.22252482175827026\n",
      "Loss:0.29187867045402527\n",
      "Loss:0.35903531312942505\n",
      "Loss:0.3491306006908417\n",
      "Loss:0.29205313324928284\n",
      "Loss:0.4462418854236603\n",
      "Loss:0.30650627613067627\n",
      "Loss:0.2065277397632599\n",
      "Loss:0.2949235439300537\n",
      "Loss:0.29462578892707825\n",
      "Loss:0.36640143394470215\n",
      "Loss:0.1612892895936966\n",
      "Loss:0.2599329650402069\n",
      "Loss:0.21280768513679504\n",
      "Loss:0.16627299785614014\n",
      "Loss:0.3543899655342102\n",
      "Loss:0.16737280786037445\n",
      "Loss:0.3203965127468109\n",
      "Loss:0.4514642059803009\n",
      "Loss:0.3409948945045471\n",
      "Loss:0.22037504613399506\n",
      "Loss:0.4462542235851288\n",
      "Loss:0.30466845631599426\n",
      "Loss:0.31070074439048767\n",
      "Loss:0.25712791085243225\n",
      "Loss:0.252035528421402\n",
      "Loss:0.4708869159221649\n",
      "Loss:0.2771878242492676\n",
      "Loss:0.29379820823669434\n",
      "Loss:0.46400707960128784\n",
      "Loss:0.24299751222133636\n",
      "Loss:0.38857245445251465\n",
      "Loss:0.403809517621994\n",
      "Loss:0.556393027305603\n",
      "Loss:0.5533022284507751\n",
      "Loss:0.19503113627433777\n",
      "Loss:0.2869884669780731\n",
      "Loss:0.3910175561904907\n",
      "Loss:0.43123114109039307\n",
      "Loss:0.2789119482040405\n",
      "Loss:0.18853414058685303\n",
      "Loss:0.3786446750164032\n",
      "Loss:0.2860492169857025\n",
      "Loss:0.24795061349868774\n",
      "Loss:0.3187792897224426\n",
      "Loss:0.29504120349884033\n",
      "Loss:0.14567503333091736\n",
      "Loss:0.1276833713054657\n",
      "Loss:0.25177618861198425\n",
      "Loss:0.27804630994796753\n",
      "Loss:0.1904091238975525\n",
      "Loss:0.18563637137413025\n",
      "Loss:0.2592662572860718\n",
      "Loss:0.15643107891082764\n",
      "Loss:0.19328446686267853\n",
      "Loss:0.5435337424278259\n",
      "Loss:0.44001471996307373\n",
      "Loss:0.359146386384964\n",
      "Loss:0.2342841476202011\n",
      "Loss:0.18905694782733917\n",
      "Loss:0.32465043663978577\n",
      "Loss:0.33947136998176575\n",
      "Loss:0.32108569145202637\n",
      "Loss:0.5183315873146057\n",
      "Loss:0.4035968780517578\n",
      "Loss:0.22921781241893768\n",
      "Loss:0.488971471786499\n",
      "Loss:0.38522300124168396\n",
      "Loss:0.21903559565544128\n",
      "Loss:0.2140384316444397\n",
      "Loss:0.21693511307239532\n",
      "Loss:0.27955150604248047\n",
      "Loss:0.3259217441082001\n",
      "Loss:0.2966473698616028\n",
      "Loss:0.4964032769203186\n",
      "Loss:0.41537728905677795\n",
      "Loss:0.42272740602493286\n",
      "Loss:0.39125651121139526\n",
      "Loss:0.355267733335495\n",
      "Loss:0.443340539932251\n",
      "Loss:0.5369443297386169\n",
      "Loss:0.31036466360092163\n",
      "Loss:0.24256472289562225\n",
      "Loss:0.4339749217033386\n",
      "Loss:0.2799573540687561\n",
      "Loss:0.21649646759033203\n",
      "Loss:0.3289719820022583\n",
      "Loss:0.2830392122268677\n",
      "Loss:0.4122934639453888\n",
      "Loss:0.32377615571022034\n",
      "Loss:0.3978317379951477\n",
      "Loss:0.23646755516529083\n",
      "Loss:0.16610276699066162\n",
      "Loss:0.24108336865901947\n",
      "Loss:0.22843037545681\n",
      "Loss:0.39346832036972046\n",
      "Loss:0.3539411425590515\n",
      "Loss:0.29810023307800293\n",
      "Loss:0.23488542437553406\n",
      "Loss:0.36140158772468567\n",
      "Loss:0.21738573908805847\n",
      "Loss:0.28517183661460876\n",
      "Loss:0.5046247839927673\n",
      "Loss:0.36145883798599243\n",
      "Loss:0.31019505858421326\n",
      "Loss:0.36005252599716187\n",
      "Loss:0.1905653327703476\n",
      "Loss:0.13548643887043\n",
      "Loss:0.30071112513542175\n",
      "Loss:0.24896815419197083\n",
      "Loss:0.31430545449256897\n",
      "Loss:0.3049389123916626\n",
      "Loss:0.22977803647518158\n",
      "Loss:0.15360619127750397\n",
      "Loss:0.28714102506637573\n",
      "Loss:0.30456453561782837\n",
      "Loss:0.2930426001548767\n",
      "Loss:0.28104785084724426\n",
      "Loss:0.1481805145740509\n",
      "Loss:0.26806655526161194\n",
      "Loss:0.3080398440361023\n",
      "Loss:0.31682875752449036\n",
      "Loss:0.35984930396080017\n",
      "Loss:0.25966665148735046\n",
      "Loss:0.11105085164308548\n",
      "Loss:0.26027417182922363\n",
      "Loss:0.41674795746803284\n",
      "Loss:0.4068785309791565\n",
      "Loss:0.5240435600280762\n",
      "Loss:0.3681755065917969\n",
      "Loss:0.3002782464027405\n",
      "Loss:0.21758262813091278\n",
      "Loss:0.26310086250305176\n",
      "Loss:0.30275803804397583\n",
      "Loss:0.2709433138370514\n",
      "Loss:0.08774498850107193\n",
      "Loss:0.24706031382083893\n",
      "Loss:0.2227889448404312\n",
      "Loss:0.35812413692474365\n",
      "Loss:0.22749948501586914\n",
      "Loss:0.3631146252155304\n",
      "Loss:0.5452133417129517\n",
      "Loss:0.23725879192352295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36mtorch_train\u001b[0;34m(dataset, model, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, label\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 16\u001b[0m, in \u001b[0;36mTorchModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch_train(dataset, torch_model, 3e-4, 100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    y_pred = model(x.to(device))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "xd torch.Size([256, 784])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(x, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(x, model):\n\u001b[0;32m----> 2\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mTorchModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x256)"
     ]
    }
   ],
   "source": [
    "print(predict(torch.tensor(x_test[0], dtype=torch.float32), torch_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyniki\n",
    "- Najlepszą dokładność na zbiorze walidacyjnym osiągnął model trzeci, z jedną ukrytą warstwą o rozmiarze 20, learning ratem o wartości 0.006 oraz batch sizem równym 128.\n",
    "- W modelu numer cztery w trakcie treningu doszło do przepełnienia.\n",
    "- Pozostałe modele osiągnęły dokładność na zbiorze walidacyjnym w granicach 80-85%.\n",
    "- Model trzeci na zbiorze testowym osiągnął wynik 93%.\n",
    "# Wnioski\n",
    "- Modele z wiekszą ilością ukrytych warstw oraz ukrytych neuronów nie zawsze osiągały wyższą dokładność. Architektura modelu powinna być ściśle dopasowana do poziomu złożoności danych - nie może być za prosta (jak w modelu 1) ani zbyt rozbudowana (jak w modelu 0).\n",
    "- Częste aktualizowanie parametrów w przypadku małego batch sizu może mieć negatywny wpływ na dokładność.\n",
    "- Mniejszy learning rate w modelu o bardziej rozbudowanej architekturze zapobiegał \"eksplozji\" gradientu.\n",
    "- Dokładność treningowa nie odbiegała zbytnio od dokładności walidacyjnej (największa różnica w modelu trzecim). Nie doszło do zjawiska zbytniego overfittingu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autodoppelganger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
