{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report from neural network task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import experiment, plot_costs\n",
    "from model_torch import TorchModel, MnistDataset, torch_train\n",
    "from preprocessing import MnistDataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "MNIST dataset is used for training and testing custom neural network. Images are loaded and preprocessed using a MnistDataLoader class from a kaggle notebook (https://www.kaggle.com/code/hojjatk/read-mnist-dataset). DataLoader class is slightly modified to return images and labels in `np.ndarray` format and split `train-images` and `train-labels` into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_train, data_val, data_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain-images-idx3-ubyte\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain-labels-idx1-ubyte\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt10k-images.idx3-ubyte\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt10k-labels.idx1-ubyte\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/experiment.py:53\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(filename_train_X, filename_train_y, filename_test_X, filename_test_y)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\n\u001b[1;32m     40\u001b[0m     filename_train_X: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     41\u001b[0m     filename_train_y: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39marray]], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39marray]], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]\n\u001b[1;32m     46\u001b[0m ]:\n\u001b[1;32m     47\u001b[0m     mnist_dataloader \u001b[38;5;241m=\u001b[39m MnistDataloader(\n\u001b[1;32m     48\u001b[0m         filename_train_X,\n\u001b[1;32m     49\u001b[0m         filename_train_y,\n\u001b[1;32m     50\u001b[0m         filename_test_X,\n\u001b[1;32m     51\u001b[0m         filename_test_y,\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 53\u001b[0m     x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m mnist_dataloader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x_train, y_train), (x_test, y_test)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "mnist_dataloader = MnistDataloader(\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\", \"t10k-images.idx3-ubyte\", \"t10k-labels.idx1-ubyte\")\n",
    "data_train, data_val, data_test = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = [[((784, 256), \"relu\"), ((256, 128), \"relu\"), ((128, 64), \"relu\"), ((64, 10), \"softmax\")],\n",
    "                  [((784, 128), \"relu\"), ((128, 64), \"relu\"), ((64, 10), \"softmax\")],\n",
    "                  [((784, 64), \"relu\"), ((64, 10), \"softmax\")],\n",
    "                  [((784, 10), \"softmax\")]]\n",
    "learning_rates_to_test = [3e-4, 3e-3, 3e-2, 3e-2]\n",
    "batch_sizes = [128, 128, 64, 64]\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.05323426211514434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_to_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rates_to_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/experiment.py:27\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(data_train, data_val, list_of_layers, num_epochs, learning_rates, batch_sizes)\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m---> 27\u001b[0m     model\u001b[38;5;241m.\u001b[39madd_module(Layer(layer[\u001b[38;5;241m0\u001b[39m], layer[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     28\u001b[0m model_losses \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m     29\u001b[0m     model, X_train, y_train, num_epochs, learning_rate, batch_size\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(model_losses)\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/model.py:63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X, y, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     batch_X \u001b[38;5;241m=\u001b[39m X[i : i \u001b[38;5;241m+\u001b[39m batch_size] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     62\u001b[0m     batch_Y \u001b[38;5;241m=\u001b[39m y[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 63\u001b[0m     loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_sum \u001b[38;5;241m/\u001b[39m (dataset_size \u001b[38;5;241m/\u001b[39m batch_size))\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/model.py:37\u001b[0m, in \u001b[0;36mModel.step\u001b[0;34m(self, X, Y, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[0;32m---> 37\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(y_pred, Y)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_propagation(y_pred, Y)\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/model.py:14\u001b[0m, in \u001b[0;36mModel.forward_propagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     12\u001b[0m A_prev \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 14\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     A_prev \u001b[38;5;241m=\u001b[39m A\n\u001b[1;32m     16\u001b[0m AL \u001b[38;5;241m=\u001b[39m A_prev\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/layer.py:18\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m---> 18\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(Z)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Z\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence-from-scratch/Zad5/layer.py:24\u001b[0m, in \u001b[0;36mLayer.forward_linear\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_linear\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     23\u001b[0m     w, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m---> 24\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Z\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, train_accuracies, val_accuracies = experiment(data_train, data_val, models_to_test,  epochs, learning_rates_to_test, batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"model\": models_to_test, \"learning_rate\": learning_rates_to_test, \"batch_size\": batch_sizes, \"train_accuracy\": train_accuracies, \"val_accuracy\": val_accuracies})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Pytorch neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = MnistDataset(np.array(y_train), np.array(x_train))\n",
    "torch_model = TorchModel(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:7.183226585388184\n",
      "Loss:5.526722431182861\n",
      "Loss:5.632143020629883\n",
      "Loss:4.806131362915039\n",
      "Loss:4.688581943511963\n",
      "Loss:4.7320780754089355\n",
      "Loss:4.388777732849121\n",
      "Loss:4.280552387237549\n",
      "Loss:3.8892598152160645\n",
      "Loss:4.133275985717773\n",
      "Loss:3.5533838272094727\n",
      "Loss:3.6376161575317383\n",
      "Loss:3.6535890102386475\n",
      "Loss:3.619192123413086\n",
      "Loss:3.247619390487671\n",
      "Loss:3.2427988052368164\n",
      "Loss:3.1567389965057373\n",
      "Loss:3.2508623600006104\n",
      "Loss:3.12253999710083\n",
      "Loss:3.07212233543396\n",
      "Loss:2.838172197341919\n",
      "Loss:2.819213628768921\n",
      "Loss:2.9339609146118164\n",
      "Loss:2.758969783782959\n",
      "Loss:2.686230182647705\n",
      "Loss:2.6710498332977295\n",
      "Loss:2.5008907318115234\n",
      "Loss:2.6712229251861572\n",
      "Loss:2.5591840744018555\n",
      "Loss:2.8358194828033447\n",
      "Loss:2.4263806343078613\n",
      "Loss:2.398364305496216\n",
      "Loss:2.4989407062530518\n",
      "Loss:2.1339433193206787\n",
      "Loss:2.5319674015045166\n",
      "Loss:2.1941096782684326\n",
      "Loss:2.3115954399108887\n",
      "Loss:2.4435923099517822\n",
      "Loss:2.187356472015381\n",
      "Loss:2.3480963706970215\n",
      "Loss:2.1209006309509277\n",
      "Loss:2.0830483436584473\n",
      "Loss:1.9194570779800415\n",
      "Loss:1.940250039100647\n",
      "Loss:1.9316866397857666\n",
      "Loss:2.0452444553375244\n",
      "Loss:1.84260892868042\n",
      "Loss:1.571102499961853\n",
      "Loss:1.8967647552490234\n",
      "Loss:1.7979180812835693\n",
      "Loss:1.858161449432373\n",
      "Loss:1.6908283233642578\n",
      "Loss:1.948098063468933\n",
      "Loss:1.951242208480835\n",
      "Loss:1.8879083395004272\n",
      "Loss:1.8074977397918701\n",
      "Loss:1.868988037109375\n",
      "Loss:1.864917516708374\n",
      "Loss:1.8677810430526733\n",
      "Loss:1.8771405220031738\n",
      "Loss:1.621315598487854\n",
      "Loss:1.9437141418457031\n",
      "Loss:1.724364995956421\n",
      "Loss:1.517279863357544\n",
      "Loss:1.666751503944397\n",
      "Loss:1.594641089439392\n",
      "Loss:1.6976864337921143\n",
      "Loss:1.6467959880828857\n",
      "Loss:1.5908998250961304\n",
      "Loss:1.7886310815811157\n",
      "Loss:1.5151454210281372\n",
      "Loss:1.638909935951233\n",
      "Loss:1.5386803150177002\n",
      "Loss:1.5574625730514526\n",
      "Loss:1.470226526260376\n",
      "Loss:1.3010462522506714\n",
      "Loss:1.53867506980896\n",
      "Loss:1.3778291940689087\n",
      "Loss:1.3157544136047363\n",
      "Loss:1.2902534008026123\n",
      "Loss:1.3092544078826904\n",
      "Loss:1.0070641040802002\n",
      "Loss:1.0567984580993652\n",
      "Loss:1.2034541368484497\n",
      "Loss:1.245116114616394\n",
      "Loss:1.1582876443862915\n",
      "Loss:1.2428866624832153\n",
      "Loss:1.3153390884399414\n",
      "Loss:1.1561537981033325\n",
      "Loss:1.1989415884017944\n",
      "Loss:1.5241546630859375\n",
      "Loss:1.3661996126174927\n",
      "Loss:1.2463725805282593\n",
      "Loss:1.265631079673767\n",
      "Loss:1.0654809474945068\n",
      "Loss:1.3144766092300415\n",
      "Loss:1.375562071800232\n",
      "Loss:1.3046196699142456\n",
      "Loss:1.3349567651748657\n",
      "Loss:1.3367185592651367\n",
      "Loss:1.2115603685379028\n",
      "Loss:1.379549503326416\n",
      "Loss:1.2986305952072144\n",
      "Loss:1.1191309690475464\n",
      "Loss:1.1034610271453857\n",
      "Loss:1.0183472633361816\n",
      "Loss:1.0464835166931152\n",
      "Loss:1.16902756690979\n",
      "Loss:1.2185572385787964\n",
      "Loss:1.4499329328536987\n",
      "Loss:1.2775241136550903\n",
      "Loss:1.2868419885635376\n",
      "Loss:1.2276626825332642\n",
      "Loss:1.1956449747085571\n",
      "Loss:1.4698883295059204\n",
      "Loss:1.4445286989212036\n",
      "Loss:1.0006178617477417\n",
      "Loss:0.9629769325256348\n",
      "Loss:1.2161998748779297\n",
      "Loss:0.9475529193878174\n",
      "Loss:0.8684853911399841\n",
      "Loss:1.0475025177001953\n",
      "Loss:0.9959689378738403\n",
      "Loss:1.384876012802124\n",
      "Loss:1.072592854499817\n",
      "Loss:1.1611316204071045\n",
      "Loss:1.0179065465927124\n",
      "Loss:0.738508403301239\n",
      "Loss:0.9158049821853638\n",
      "Loss:0.8162673115730286\n",
      "Loss:1.0849486589431763\n",
      "Loss:1.176273226737976\n",
      "Loss:1.157070517539978\n",
      "Loss:1.010160207748413\n",
      "Loss:1.1420035362243652\n",
      "Loss:0.9796024560928345\n",
      "Loss:1.0102097988128662\n",
      "Loss:1.2057968378067017\n",
      "Loss:1.32492196559906\n",
      "Loss:1.0517085790634155\n",
      "Loss:0.9579688906669617\n",
      "Loss:0.804420530796051\n",
      "Loss:0.663645327091217\n",
      "Loss:1.1863222122192383\n",
      "Loss:1.0025635957717896\n",
      "Loss:1.0123196840286255\n",
      "Loss:0.8666189312934875\n",
      "Loss:0.7621559500694275\n",
      "Loss:0.8110678195953369\n",
      "Loss:1.0701948404312134\n",
      "Loss:0.9630576372146606\n",
      "Loss:0.9235200881958008\n",
      "Loss:0.7299878597259521\n",
      "Loss:0.5975531339645386\n",
      "Loss:0.858187198638916\n",
      "Loss:0.9719164967536926\n",
      "Loss:0.9400126338005066\n",
      "Loss:0.8549361824989319\n",
      "Loss:0.8220609426498413\n",
      "Loss:0.7659251093864441\n",
      "Loss:0.7834504246711731\n",
      "Loss:0.7946643829345703\n",
      "Loss:1.0062270164489746\n",
      "Loss:0.9496512413024902\n",
      "Loss:0.8164792060852051\n",
      "Loss:0.7150620222091675\n",
      "Loss:0.769107460975647\n",
      "Loss:0.869174599647522\n",
      "Loss:0.8067666888237\n",
      "Loss:0.7891886234283447\n",
      "Loss:0.5569398999214172\n",
      "Loss:0.5321087837219238\n",
      "Loss:0.9021697640419006\n",
      "Loss:1.0132761001586914\n",
      "Loss:0.7903534770011902\n",
      "Loss:1.029099941253662\n",
      "Loss:1.1551295518875122\n",
      "Loss:0.7800341844558716\n",
      "Loss:0.8170403242111206\n",
      "Loss:0.6505815386772156\n",
      "Loss:0.815127968788147\n",
      "Loss:0.7640226483345032\n",
      "Loss:0.7136472463607788\n",
      "Loss:0.7471678853034973\n",
      "Loss:0.8870394229888916\n",
      "Loss:0.9490110874176025\n",
      "Loss:0.863248884677887\n",
      "Loss:0.7375642657279968\n",
      "Loss:0.5646207928657532\n",
      "Loss:0.8785293698310852\n",
      "Loss:0.6704815626144409\n",
      "Loss:0.8830004930496216\n",
      "Loss:0.9851299524307251\n",
      "Loss:0.8750880360603333\n",
      "Loss:0.8545752167701721\n",
      "Loss:0.7397692203521729\n",
      "Loss:0.83051598072052\n",
      "Loss:0.9050266146659851\n",
      "Loss:0.5097472667694092\n",
      "Loss:0.642388105392456\n",
      "Loss:0.524403989315033\n",
      "Loss:0.8951879143714905\n",
      "Loss:0.8238120079040527\n",
      "Loss:0.7081436514854431\n",
      "Loss:0.655464231967926\n",
      "Loss:0.7115350365638733\n",
      "Loss:0.9828740954399109\n",
      "Loss:0.9021883606910706\n",
      "Loss:0.9200114607810974\n",
      "Loss:0.6759307384490967\n",
      "Loss:0.6439993977546692\n",
      "Loss:0.6694997549057007\n",
      "Loss:0.983368456363678\n",
      "Loss:0.7422537803649902\n",
      "Loss:0.9126206040382385\n",
      "Loss:0.588567316532135\n",
      "Loss:0.7137417197227478\n",
      "Loss:0.6908871531486511\n",
      "Loss:0.555438220500946\n",
      "Loss:0.6112912893295288\n",
      "Loss:0.7720237970352173\n",
      "Loss:0.7784655690193176\n",
      "Loss:0.7295743823051453\n",
      "Loss:0.8356872797012329\n",
      "Loss:0.6262772083282471\n",
      "Loss:0.6884726881980896\n",
      "Loss:0.6874209046363831\n",
      "Loss:0.9443842172622681\n",
      "Loss:0.611572265625\n",
      "Loss:0.6420198082923889\n",
      "Loss:0.5987276434898376\n",
      "Loss:0.7914502620697021\n",
      "Loss:0.8278701305389404\n",
      "Loss:0.8915467262268066\n",
      "Loss:0.8784888386726379\n",
      "Loss:1.0547558069229126\n",
      "Loss:0.608161449432373\n",
      "Loss:0.5825793147087097\n",
      "Loss:0.8518680930137634\n",
      "Loss:0.9071235060691833\n",
      "Loss:0.747666597366333\n",
      "Loss:0.9059325456619263\n",
      "Loss:0.7615599036216736\n",
      "Loss:0.8525334596633911\n",
      "Loss:0.9472617506980896\n",
      "Loss:0.8453592658042908\n",
      "Loss:0.7312651872634888\n",
      "Loss:1.0687025785446167\n",
      "Loss:0.7455534934997559\n",
      "Loss:0.8189125061035156\n",
      "Loss:0.661171555519104\n",
      "Loss:0.8875608444213867\n",
      "Loss:0.831480860710144\n",
      "Loss:0.8791706562042236\n",
      "Loss:0.5327625870704651\n",
      "Loss:0.6693947911262512\n",
      "Loss:0.7665755152702332\n",
      "Loss:0.5616053342819214\n",
      "Loss:0.8305230736732483\n",
      "Loss:0.6374946236610413\n",
      "Loss:0.8359099626541138\n",
      "Loss:0.8297848701477051\n",
      "Loss:0.6889629364013672\n",
      "Loss:0.5330284833908081\n",
      "Loss:0.33640527725219727\n",
      "Loss:0.4616556763648987\n",
      "Loss:0.5804249048233032\n",
      "Loss:0.49405863881111145\n",
      "Loss:0.6615365147590637\n",
      "Loss:0.7726799249649048\n",
      "Loss:0.6893777847290039\n",
      "Loss:0.747215211391449\n",
      "Loss:0.7916151881217957\n",
      "Loss:0.4831092953681946\n",
      "Loss:0.6380133032798767\n",
      "Loss:0.6457613110542297\n",
      "Loss:0.5886660814285278\n",
      "Loss:0.5826389193534851\n",
      "Loss:0.6588478088378906\n",
      "Loss:0.5757371187210083\n",
      "Loss:0.6808918714523315\n",
      "Loss:0.8409587740898132\n",
      "Loss:0.6305530071258545\n",
      "Loss:0.4413718581199646\n",
      "Loss:0.7244616150856018\n",
      "Loss:0.4759594202041626\n",
      "Loss:0.4443264305591583\n",
      "Loss:0.6472370028495789\n",
      "Loss:0.5662783980369568\n",
      "Loss:0.609960675239563\n",
      "Loss:0.6254696249961853\n",
      "Loss:0.7653011083602905\n",
      "Loss:1.078884482383728\n",
      "Loss:0.699604332447052\n",
      "Loss:0.7553534507751465\n",
      "Loss:0.7397436499595642\n",
      "Loss:0.5872997641563416\n",
      "Loss:0.5245137214660645\n",
      "Loss:0.6244034171104431\n",
      "Loss:0.7022801041603088\n",
      "Loss:0.7205820679664612\n",
      "Loss:0.7095913290977478\n",
      "Loss:0.5734535455703735\n",
      "Loss:0.4294133186340332\n",
      "Loss:0.5743480920791626\n",
      "Loss:0.4474039375782013\n",
      "Loss:0.5415765047073364\n",
      "Loss:0.9676219820976257\n",
      "Loss:0.6880576014518738\n",
      "Loss:0.5786920785903931\n",
      "Loss:0.7100452184677124\n",
      "Loss:0.897841215133667\n",
      "Loss:0.770685613155365\n",
      "Loss:0.5792362689971924\n",
      "Loss:0.6089088916778564\n",
      "Loss:0.4863210916519165\n",
      "Loss:0.6055013537406921\n",
      "Loss:0.6344646215438843\n",
      "Loss:0.5065963864326477\n",
      "Loss:0.46111008524894714\n",
      "Loss:0.6866326928138733\n",
      "Loss:0.5472661256790161\n",
      "Loss:0.7866488695144653\n",
      "Loss:0.8768854141235352\n",
      "Loss:0.7689141631126404\n",
      "Loss:0.5805044770240784\n",
      "Loss:0.6925962567329407\n",
      "Loss:0.6098331212997437\n",
      "Loss:0.5331023335456848\n",
      "Loss:0.6836220026016235\n",
      "Loss:0.8215294480323792\n",
      "Loss:0.80368572473526\n",
      "Loss:0.584113359451294\n",
      "Loss:0.5907378196716309\n",
      "Loss:0.5917882323265076\n",
      "Loss:0.6388505697250366\n",
      "Loss:0.8019914031028748\n",
      "Loss:0.4804759919643402\n",
      "Loss:0.33214864134788513\n",
      "Loss:0.3377046287059784\n",
      "Loss:0.4913351535797119\n",
      "Loss:0.5129700899124146\n",
      "Loss:0.635035514831543\n",
      "Loss:0.6564646363258362\n",
      "Loss:0.7861446738243103\n",
      "Loss:0.6703380346298218\n",
      "Loss:0.7219785451889038\n",
      "Loss:0.514081597328186\n",
      "Loss:0.3806295394897461\n",
      "Loss:0.45786693692207336\n",
      "Loss:0.5543138980865479\n",
      "Loss:0.6778643727302551\n",
      "Loss:0.6532737016677856\n",
      "Loss:0.47491809725761414\n",
      "Loss:0.5428684949874878\n",
      "Loss:0.7613574266433716\n",
      "Loss:0.659593939781189\n",
      "Loss:0.4852123260498047\n",
      "Loss:0.6901792883872986\n",
      "Loss:0.7376331686973572\n",
      "Loss:0.6917116641998291\n",
      "Loss:0.6906830668449402\n",
      "Loss:0.7556953430175781\n",
      "Loss:0.41285428404808044\n",
      "Loss:0.4808458089828491\n",
      "Loss:0.5074493288993835\n",
      "Loss:0.4935303032398224\n",
      "Loss:0.5665197968482971\n",
      "Loss:0.5251901149749756\n",
      "Loss:0.7090660333633423\n",
      "Loss:0.6111735105514526\n",
      "Loss:0.5602843165397644\n",
      "Loss:0.545633852481842\n",
      "Loss:0.49372339248657227\n",
      "Loss:0.7012952566146851\n",
      "Loss:0.3848971426486969\n",
      "Loss:0.32159560918807983\n",
      "Loss:0.5811554193496704\n",
      "Loss:0.46391797065734863\n",
      "Loss:0.6042945981025696\n",
      "Loss:0.404837965965271\n",
      "Loss:0.3752959966659546\n",
      "Loss:0.9267938137054443\n",
      "Loss:0.7206288576126099\n",
      "Loss:0.7819652557373047\n",
      "Loss:0.3594895303249359\n",
      "Loss:0.8282905220985413\n",
      "Loss:0.8519145846366882\n",
      "Loss:0.4591549336910248\n",
      "Loss:0.5980850458145142\n",
      "Loss:0.5534780621528625\n",
      "Loss:0.5371209979057312\n",
      "Loss:0.5912230610847473\n",
      "Loss:0.9725303649902344\n",
      "Loss:0.47737807035446167\n",
      "Loss:0.6954976320266724\n",
      "Loss:0.4656452238559723\n",
      "Loss:0.455249160528183\n",
      "Loss:0.3845629096031189\n",
      "Loss:0.5367560386657715\n",
      "Loss:0.5900022983551025\n",
      "Loss:0.39029696583747864\n",
      "Loss:0.41875365376472473\n",
      "Loss:0.42022615671157837\n",
      "Loss:0.5222014784812927\n",
      "Loss:0.46593400835990906\n",
      "Loss:0.6444782018661499\n",
      "Loss:0.8185267448425293\n",
      "Loss:0.6322590112686157\n",
      "Loss:0.5066956281661987\n",
      "Loss:0.3409033417701721\n",
      "Loss:0.49171921610832214\n",
      "Loss:0.7421106696128845\n",
      "Loss:1.0047775506973267\n",
      "Loss:0.5059687495231628\n",
      "Loss:0.6509966254234314\n",
      "Loss:0.41422000527381897\n",
      "Loss:0.3963237404823303\n",
      "Loss:0.6427357792854309\n",
      "Loss:0.42682361602783203\n",
      "Loss:0.4684964418411255\n",
      "Loss:0.6648097038269043\n",
      "Loss:0.6730256676673889\n",
      "Loss:0.4123383164405823\n",
      "Loss:0.4972296357154846\n",
      "Loss:0.38423505425453186\n",
      "Loss:0.4855295419692993\n",
      "Loss:0.34469500184059143\n",
      "Loss:0.6053139567375183\n",
      "Loss:0.7258485555648804\n",
      "Loss:0.380306601524353\n",
      "Loss:0.4016439616680145\n",
      "Loss:0.4166547954082489\n",
      "Loss:0.5170087814331055\n",
      "Loss:0.41049179434776306\n",
      "Loss:0.4783386290073395\n",
      "Loss:0.3349243700504303\n",
      "Loss:0.3061423897743225\n",
      "Loss:0.3999296724796295\n",
      "Loss:0.5486968159675598\n",
      "Loss:0.36832839250564575\n",
      "Loss:0.6085433959960938\n",
      "Loss:0.5858242511749268\n",
      "Loss:0.39744317531585693\n",
      "Loss:0.3842085003852844\n",
      "Loss:0.5720070004463196\n",
      "Loss:0.38063061237335205\n",
      "Loss:0.577468752861023\n",
      "Loss:0.4259199798107147\n",
      "Loss:0.5621985197067261\n",
      "Loss:0.6859099864959717\n",
      "Loss:0.46793586015701294\n",
      "Loss:0.4630887806415558\n",
      "Loss:0.4521748125553131\n",
      "Loss:0.30913498997688293\n",
      "Loss:0.3329761326313019\n",
      "Loss:0.43482834100723267\n",
      "Loss:0.45376089215278625\n",
      "Loss:0.3725982904434204\n",
      "Loss:0.4100576937198639\n",
      "Loss:0.16967114806175232\n",
      "Loss:0.1364615261554718\n",
      "Loss:0.2847353219985962\n",
      "Loss:0.5630621314048767\n",
      "Loss:0.32857418060302734\n",
      "Loss:0.24585850536823273\n",
      "Loss:0.7627518773078918\n",
      "Loss:0.1742863953113556\n",
      "Loss:0.5653350353240967\n",
      "Epoch:0, Loss: 0.5653350353240967\n",
      "Loss:0.4574030339717865\n",
      "Loss:0.519801676273346\n",
      "Loss:0.3515865206718445\n",
      "Loss:0.4982525408267975\n",
      "Loss:0.6467849016189575\n",
      "Loss:0.5100286602973938\n",
      "Loss:0.6105766296386719\n",
      "Loss:0.5688639283180237\n",
      "Loss:0.8464309573173523\n",
      "Loss:0.3655546307563782\n",
      "Loss:0.5207856893539429\n",
      "Loss:0.4793182611465454\n",
      "Loss:0.5290214419364929\n",
      "Loss:0.3706029951572418\n",
      "Loss:0.3754388093948364\n",
      "Loss:0.42776963114738464\n",
      "Loss:0.3806871771812439\n",
      "Loss:0.4699302315711975\n",
      "Loss:0.4588819444179535\n",
      "Loss:0.3183659315109253\n",
      "Loss:0.4635468125343323\n",
      "Loss:0.48469001054763794\n",
      "Loss:0.5068405866622925\n",
      "Loss:0.533284604549408\n",
      "Loss:0.3813386559486389\n",
      "Loss:0.4198840260505676\n",
      "Loss:0.38416528701782227\n",
      "Loss:0.47186079621315\n",
      "Loss:0.4219749867916107\n",
      "Loss:0.5567904710769653\n",
      "Loss:0.37727537751197815\n",
      "Loss:0.46396124362945557\n",
      "Loss:0.5258429050445557\n",
      "Loss:0.48903271555900574\n",
      "Loss:0.4111887514591217\n",
      "Loss:0.3756652772426605\n",
      "Loss:0.48769211769104004\n",
      "Loss:0.5179304480552673\n",
      "Loss:0.49921202659606934\n",
      "Loss:0.47266843914985657\n",
      "Loss:0.6616159081459045\n",
      "Loss:0.45462045073509216\n",
      "Loss:0.34001219272613525\n",
      "Loss:0.49883148074150085\n",
      "Loss:0.4328042268753052\n",
      "Loss:0.5289206504821777\n",
      "Loss:0.32626593112945557\n",
      "Loss:0.36917048692703247\n",
      "Loss:0.39267775416374207\n",
      "Loss:0.30499663949012756\n",
      "Loss:0.5089313983917236\n",
      "Loss:0.31432104110717773\n",
      "Loss:0.4318765103816986\n",
      "Loss:0.6371269822120667\n",
      "Loss:0.5394434928894043\n",
      "Loss:0.36481353640556335\n",
      "Loss:0.6690051555633545\n",
      "Loss:0.497917503118515\n",
      "Loss:0.5133903622627258\n",
      "Loss:0.41602176427841187\n",
      "Loss:0.42583584785461426\n",
      "Loss:0.6809587478637695\n",
      "Loss:0.484031617641449\n",
      "Loss:0.4458021819591522\n",
      "Loss:0.6206291913986206\n",
      "Loss:0.42820194363594055\n",
      "Loss:0.5298492908477783\n",
      "Loss:0.607647716999054\n",
      "Loss:0.6700934171676636\n",
      "Loss:0.7323208451271057\n",
      "Loss:0.3285825848579407\n",
      "Loss:0.42197656631469727\n",
      "Loss:0.5897340178489685\n",
      "Loss:0.589164674282074\n",
      "Loss:0.407027006149292\n",
      "Loss:0.31818926334381104\n",
      "Loss:0.5920013189315796\n",
      "Loss:0.4215611517429352\n",
      "Loss:0.38229623436927795\n",
      "Loss:0.43928852677345276\n",
      "Loss:0.48910972476005554\n",
      "Loss:0.2512248754501343\n",
      "Loss:0.2825828194618225\n",
      "Loss:0.4003627300262451\n",
      "Loss:0.39397042989730835\n",
      "Loss:0.33713406324386597\n",
      "Loss:0.31704607605934143\n",
      "Loss:0.422439843416214\n",
      "Loss:0.3053503930568695\n",
      "Loss:0.3089302182197571\n",
      "Loss:0.757408857345581\n",
      "Loss:0.6405020952224731\n",
      "Loss:0.5217147469520569\n",
      "Loss:0.3762243688106537\n",
      "Loss:0.34624162316322327\n",
      "Loss:0.5062709450721741\n",
      "Loss:0.5233845114707947\n",
      "Loss:0.5420333743095398\n",
      "Loss:0.6837558150291443\n",
      "Loss:0.5861386060714722\n",
      "Loss:0.35837322473526\n",
      "Loss:0.7243540287017822\n",
      "Loss:0.5832046866416931\n",
      "Loss:0.37606313824653625\n",
      "Loss:0.36609986424446106\n",
      "Loss:0.35746198892593384\n",
      "Loss:0.42581045627593994\n",
      "Loss:0.4787031412124634\n",
      "Loss:0.46183016896247864\n",
      "Loss:0.7284295558929443\n",
      "Loss:0.6097449064254761\n",
      "Loss:0.6332493424415588\n",
      "Loss:0.5647604465484619\n",
      "Loss:0.5566654205322266\n",
      "Loss:0.6924347281455994\n",
      "Loss:0.7325791716575623\n",
      "Loss:0.4731270372867584\n",
      "Loss:0.3722235858440399\n",
      "Loss:0.5858439803123474\n",
      "Loss:0.4075992703437805\n",
      "Loss:0.326884925365448\n",
      "Loss:0.4812571406364441\n",
      "Loss:0.4286516308784485\n",
      "Loss:0.6446443796157837\n",
      "Loss:0.4814226031303406\n",
      "Loss:0.5859799385070801\n",
      "Loss:0.42978790402412415\n",
      "Loss:0.2412029206752777\n",
      "Loss:0.35526591539382935\n",
      "Loss:0.31334763765335083\n",
      "Loss:0.5637363791465759\n",
      "Loss:0.5130883455276489\n",
      "Loss:0.48669278621673584\n",
      "Loss:0.4101785123348236\n",
      "Loss:0.5711360573768616\n",
      "Loss:0.37760457396507263\n",
      "Loss:0.4376653730869293\n",
      "Loss:0.6901385188102722\n",
      "Loss:0.5701647400856018\n",
      "Loss:0.4141579866409302\n",
      "Loss:0.47438374161720276\n",
      "Loss:0.3132465183734894\n",
      "Loss:0.23112809658050537\n",
      "Loss:0.4977107644081116\n",
      "Loss:0.37952736020088196\n",
      "Loss:0.46655505895614624\n",
      "Loss:0.41601210832595825\n",
      "Loss:0.33362317085266113\n",
      "Loss:0.2852258086204529\n",
      "Loss:0.4777974784374237\n",
      "Loss:0.4715079963207245\n",
      "Loss:0.44381004571914673\n",
      "Loss:0.3876805007457733\n",
      "Loss:0.23241117596626282\n",
      "Loss:0.3923950791358948\n",
      "Loss:0.44775164127349854\n",
      "Loss:0.46434032917022705\n",
      "Loss:0.48171335458755493\n",
      "Loss:0.3746032416820526\n",
      "Loss:0.23138339817523956\n",
      "Loss:0.3589074909687042\n",
      "Loss:0.49814799427986145\n",
      "Loss:0.575086772441864\n",
      "Loss:0.6034654974937439\n",
      "Loss:0.4733760356903076\n",
      "Loss:0.3657269775867462\n",
      "Loss:0.3209017515182495\n",
      "Loss:0.4211151599884033\n",
      "Loss:0.4403589069843292\n",
      "Loss:0.40184903144836426\n",
      "Loss:0.20113973319530487\n",
      "Loss:0.32074376940727234\n",
      "Loss:0.4020366072654724\n",
      "Loss:0.5286664962768555\n",
      "Loss:0.36432144045829773\n",
      "Loss:0.5635953545570374\n",
      "Loss:0.6606077551841736\n",
      "Loss:0.3887665271759033\n",
      "Loss:0.38909006118774414\n",
      "Loss:0.2890903651714325\n",
      "Loss:0.41920745372772217\n",
      "Loss:0.31931766867637634\n",
      "Loss:0.3624105453491211\n",
      "Loss:0.36670982837677\n",
      "Loss:0.46036529541015625\n",
      "Loss:0.47213083505630493\n",
      "Loss:0.4338599443435669\n",
      "Loss:0.402814120054245\n",
      "Loss:0.26177966594696045\n",
      "Loss:0.4845258593559265\n",
      "Loss:0.3142394721508026\n",
      "Loss:0.5370665192604065\n",
      "Loss:0.5154367089271545\n",
      "Loss:0.4906957745552063\n",
      "Loss:0.5050980448722839\n",
      "Loss:0.35715773701667786\n",
      "Loss:0.44164741039276123\n",
      "Loss:0.5562328100204468\n",
      "Loss:0.21290802955627441\n",
      "Loss:0.38280147314071655\n",
      "Loss:0.2668914198875427\n",
      "Loss:0.5236836671829224\n",
      "Loss:0.41120532155036926\n",
      "Loss:0.3343237638473511\n",
      "Loss:0.3653758466243744\n",
      "Loss:0.3893597722053528\n",
      "Loss:0.6205509305000305\n",
      "Loss:0.5118566751480103\n",
      "Loss:0.6607396602630615\n",
      "Loss:0.3792494237422943\n",
      "Loss:0.3536570072174072\n",
      "Loss:0.27519065141677856\n",
      "Loss:0.6786314249038696\n",
      "Loss:0.37164586782455444\n",
      "Loss:0.5179852247238159\n",
      "Loss:0.32741430401802063\n",
      "Loss:0.3889857232570648\n",
      "Loss:0.4311797320842743\n",
      "Loss:0.3188852369785309\n",
      "Loss:0.3351355195045471\n",
      "Loss:0.45853012800216675\n",
      "Loss:0.5184957981109619\n",
      "Loss:0.48761022090911865\n",
      "Loss:0.617925763130188\n",
      "Loss:0.34592676162719727\n",
      "Loss:0.32372432947158813\n",
      "Loss:0.3657192587852478\n",
      "Loss:0.5962235927581787\n",
      "Loss:0.3476848006248474\n",
      "Loss:0.4278404414653778\n",
      "Loss:0.303295761346817\n",
      "Loss:0.46068069338798523\n",
      "Loss:0.4898724853992462\n",
      "Loss:0.5981444716453552\n",
      "Loss:0.5467545986175537\n",
      "Loss:0.6001569628715515\n",
      "Loss:0.36750638484954834\n",
      "Loss:0.32160162925720215\n",
      "Loss:0.47784802317619324\n",
      "Loss:0.564202606678009\n",
      "Loss:0.46034055948257446\n",
      "Loss:0.5662992596626282\n",
      "Loss:0.44703730940818787\n",
      "Loss:0.5680413246154785\n",
      "Loss:0.6830921769142151\n",
      "Loss:0.5132134556770325\n",
      "Loss:0.4601832926273346\n",
      "Loss:0.76454758644104\n",
      "Loss:0.4126025140285492\n",
      "Loss:0.4698607325553894\n",
      "Loss:0.4929414391517639\n",
      "Loss:0.5430234670639038\n",
      "Loss:0.4953359365463257\n",
      "Loss:0.6173672676086426\n",
      "Loss:0.24412094056606293\n",
      "Loss:0.45118096470832825\n",
      "Loss:0.48910489678382874\n",
      "Loss:0.3046413064002991\n",
      "Loss:0.4933033883571625\n",
      "Loss:0.33586597442626953\n",
      "Loss:0.5205479860305786\n",
      "Loss:0.5013739466667175\n",
      "Loss:0.36454325914382935\n",
      "Loss:0.3192828595638275\n",
      "Loss:0.16046299040317535\n",
      "Loss:0.24389879405498505\n",
      "Loss:0.3387637138366699\n",
      "Loss:0.24962027370929718\n",
      "Loss:0.43624943494796753\n",
      "Loss:0.5088284611701965\n",
      "Loss:0.44293922185897827\n",
      "Loss:0.4405185580253601\n",
      "Loss:0.4987027049064636\n",
      "Loss:0.26199889183044434\n",
      "Loss:0.3478914797306061\n",
      "Loss:0.41118142008781433\n",
      "Loss:0.31447890400886536\n",
      "Loss:0.367959588766098\n",
      "Loss:0.3980468809604645\n",
      "Loss:0.3236061930656433\n",
      "Loss:0.4063957631587982\n",
      "Loss:0.4824356436729431\n",
      "Loss:0.3942354619503021\n",
      "Loss:0.23646694421768188\n",
      "Loss:0.5272694230079651\n",
      "Loss:0.2702781558036804\n",
      "Loss:0.2377222180366516\n",
      "Loss:0.4363889694213867\n",
      "Loss:0.31890541315078735\n",
      "Loss:0.41666772961616516\n",
      "Loss:0.4333786964416504\n",
      "Loss:0.5864399671554565\n",
      "Loss:0.837783932685852\n",
      "Loss:0.4571446478366852\n",
      "Loss:0.5280778408050537\n",
      "Loss:0.4631763994693756\n",
      "Loss:0.38943082094192505\n",
      "Loss:0.2950344681739807\n",
      "Loss:0.4007539451122284\n",
      "Loss:0.4552096426486969\n",
      "Loss:0.4556828737258911\n",
      "Loss:0.44073405861854553\n",
      "Loss:0.3528844118118286\n",
      "Loss:0.22411474585533142\n",
      "Loss:0.3379665017127991\n",
      "Loss:0.27271848917007446\n",
      "Loss:0.3390823304653168\n",
      "Loss:0.7482727766036987\n",
      "Loss:0.45833203196525574\n",
      "Loss:0.3674975335597992\n",
      "Loss:0.3838048279285431\n",
      "Loss:0.6002121567726135\n",
      "Loss:0.5177383422851562\n",
      "Loss:0.34532877802848816\n",
      "Loss:0.41752445697784424\n",
      "Loss:0.29939767718315125\n",
      "Loss:0.36367011070251465\n",
      "Loss:0.41419920325279236\n",
      "Loss:0.31235358119010925\n",
      "Loss:0.26703596115112305\n",
      "Loss:0.49483147263526917\n",
      "Loss:0.3082158863544464\n",
      "Loss:0.5970125794410706\n",
      "Loss:0.6615098118782043\n",
      "Loss:0.48676013946533203\n",
      "Loss:0.3471127450466156\n",
      "Loss:0.4692946672439575\n",
      "Loss:0.4118032157421112\n",
      "Loss:0.37679266929626465\n",
      "Loss:0.4028548300266266\n",
      "Loss:0.5257084965705872\n",
      "Loss:0.5721365809440613\n",
      "Loss:0.41852644085884094\n",
      "Loss:0.4166897237300873\n",
      "Loss:0.45262131094932556\n",
      "Loss:0.47806060314178467\n",
      "Loss:0.5205724239349365\n",
      "Loss:0.3043597638607025\n",
      "Loss:0.19049912691116333\n",
      "Loss:0.2017565369606018\n",
      "Loss:0.35266125202178955\n",
      "Loss:0.3269168734550476\n",
      "Loss:0.43427497148513794\n",
      "Loss:0.4357323944568634\n",
      "Loss:0.5093468427658081\n",
      "Loss:0.42870649695396423\n",
      "Loss:0.5089539289474487\n",
      "Loss:0.37328341603279114\n",
      "Loss:0.22418531775474548\n",
      "Loss:0.29029595851898193\n",
      "Loss:0.3760043978691101\n",
      "Loss:0.5234304070472717\n",
      "Loss:0.44397521018981934\n",
      "Loss:0.31843242049217224\n",
      "Loss:0.3331306576728821\n",
      "Loss:0.5769564509391785\n",
      "Loss:0.4628831744194031\n",
      "Loss:0.3570513129234314\n",
      "Loss:0.5067009925842285\n",
      "Loss:0.5426564812660217\n",
      "Loss:0.47907713055610657\n",
      "Loss:0.5212595462799072\n",
      "Loss:0.5676192045211792\n",
      "Loss:0.2468724101781845\n",
      "Loss:0.28337565064430237\n",
      "Loss:0.35950443148612976\n",
      "Loss:0.3279705345630646\n",
      "Loss:0.36951664090156555\n",
      "Loss:0.34508126974105835\n",
      "Loss:0.4937077462673187\n",
      "Loss:0.41410186886787415\n",
      "Loss:0.3750351071357727\n",
      "Loss:0.3962506949901581\n",
      "Loss:0.32201412320137024\n",
      "Loss:0.5466375946998596\n",
      "Loss:0.23902086913585663\n",
      "Loss:0.20470471680164337\n",
      "Loss:0.47120749950408936\n",
      "Loss:0.3132466673851013\n",
      "Loss:0.39829131960868835\n",
      "Loss:0.23934544622898102\n",
      "Loss:0.23502543568611145\n",
      "Loss:0.7630252838134766\n",
      "Loss:0.5268656611442566\n",
      "Loss:0.5342406630516052\n",
      "Loss:0.19281020760536194\n",
      "Loss:0.6218231916427612\n",
      "Loss:0.6824682354927063\n",
      "Loss:0.3277568817138672\n",
      "Loss:0.43083199858665466\n",
      "Loss:0.3737923204898834\n",
      "Loss:0.3871336579322815\n",
      "Loss:0.4577365815639496\n",
      "Loss:0.7960633635520935\n",
      "Loss:0.30967602133750916\n",
      "Loss:0.5138425230979919\n",
      "Loss:0.29947543144226074\n",
      "Loss:0.2851804494857788\n",
      "Loss:0.2299966812133789\n",
      "Loss:0.354325532913208\n",
      "Loss:0.4264964461326599\n",
      "Loss:0.2592138648033142\n",
      "Loss:0.2935135066509247\n",
      "Loss:0.2801455557346344\n",
      "Loss:0.35492730140686035\n",
      "Loss:0.33133161067962646\n",
      "Loss:0.5055174827575684\n",
      "Loss:0.5922573208808899\n",
      "Loss:0.4692005217075348\n",
      "Loss:0.3918394148349762\n",
      "Loss:0.2164948582649231\n",
      "Loss:0.3900669813156128\n",
      "Loss:0.5603400468826294\n",
      "Loss:0.8423532247543335\n",
      "Loss:0.3559301197528839\n",
      "Loss:0.4927622973918915\n",
      "Loss:0.25790494680404663\n",
      "Loss:0.2353483885526657\n",
      "Loss:0.4583154022693634\n",
      "Loss:0.3006523549556732\n",
      "Loss:0.3353787660598755\n",
      "Loss:0.5159235000610352\n",
      "Loss:0.45692160725593567\n",
      "Loss:0.29441994428634644\n",
      "Loss:0.36386221647262573\n",
      "Loss:0.24480217695236206\n",
      "Loss:0.35787340998649597\n",
      "Loss:0.2377982884645462\n",
      "Loss:0.46689972281455994\n",
      "Loss:0.5524165034294128\n",
      "Loss:0.2558940351009369\n",
      "Loss:0.31485632061958313\n",
      "Loss:0.33334848284721375\n",
      "Loss:0.3474050462245941\n",
      "Loss:0.28088071942329407\n",
      "Loss:0.3891768753528595\n",
      "Loss:0.23112857341766357\n",
      "Loss:0.20395854115486145\n",
      "Loss:0.2431292086839676\n",
      "Loss:0.41994673013687134\n",
      "Loss:0.24673326313495636\n",
      "Loss:0.4696558117866516\n",
      "Loss:0.4343905746936798\n",
      "Loss:0.2637070119380951\n",
      "Loss:0.2564227283000946\n",
      "Loss:0.40149956941604614\n",
      "Loss:0.2276497781276703\n",
      "Loss:0.3944894075393677\n",
      "Loss:0.2580156922340393\n",
      "Loss:0.36887237429618835\n",
      "Loss:0.5063408613204956\n",
      "Loss:0.3323357105255127\n",
      "Loss:0.32275232672691345\n",
      "Loss:0.3552711606025696\n",
      "Loss:0.20919162034988403\n",
      "Loss:0.23673485219478607\n",
      "Loss:0.3066518008708954\n",
      "Loss:0.2964174747467041\n",
      "Loss:0.22542297840118408\n",
      "Loss:0.2883095443248749\n",
      "Loss:0.09900318831205368\n",
      "Loss:0.061855122447013855\n",
      "Loss:0.19550065696239471\n",
      "Loss:0.4249860942363739\n",
      "Loss:0.2293599247932434\n",
      "Loss:0.13237932324409485\n",
      "Loss:0.6550478935241699\n",
      "Loss:0.10297829657793045\n",
      "Loss:0.422017365694046\n",
      "Loss:0.34881120920181274\n",
      "Loss:0.37828877568244934\n",
      "Loss:0.25244662165641785\n",
      "Loss:0.395783007144928\n",
      "Loss:0.49872642755508423\n",
      "Loss:0.3597423732280731\n",
      "Loss:0.4716213047504425\n",
      "Loss:0.4261445999145508\n",
      "Loss:0.6953111290931702\n",
      "Loss:0.27706247568130493\n",
      "Loss:0.4549432694911957\n",
      "Loss:0.34340715408325195\n",
      "Loss:0.38546326756477356\n",
      "Loss:0.2723637521266937\n",
      "Loss:0.258482426404953\n",
      "Loss:0.2963896691799164\n",
      "Loss:0.28527316451072693\n",
      "Loss:0.3571801781654358\n",
      "Loss:0.35227030515670776\n",
      "Loss:0.1999448537826538\n",
      "Loss:0.36017969250679016\n",
      "Loss:0.36786073446273804\n",
      "Loss:0.3732110857963562\n",
      "Loss:0.419860303401947\n",
      "Loss:0.2508709132671356\n",
      "Loss:0.3246332108974457\n",
      "Loss:0.2788326144218445\n",
      "Loss:0.3362058103084564\n",
      "Loss:0.347659707069397\n",
      "Loss:0.4163607954978943\n",
      "Loss:0.24241673946380615\n",
      "Loss:0.35333317518234253\n",
      "Loss:0.4139055609703064\n",
      "Loss:0.3653155267238617\n",
      "Loss:0.27364474534988403\n",
      "Loss:0.2672352194786072\n",
      "Loss:0.35293033719062805\n",
      "Loss:0.4093400835990906\n",
      "Loss:0.3969338536262512\n",
      "Loss:0.3451591730117798\n",
      "Loss:0.521457850933075\n",
      "Loss:0.3545738160610199\n",
      "Loss:0.2438724786043167\n",
      "Loss:0.3665125072002411\n",
      "Loss:0.34410449862480164\n",
      "Loss:0.42312395572662354\n",
      "Loss:0.20782704651355743\n",
      "Loss:0.30375418066978455\n",
      "Loss:0.2653176784515381\n",
      "Loss:0.2052401453256607\n",
      "Loss:0.40488162636756897\n",
      "Loss:0.21212317049503326\n",
      "Loss:0.3586142957210541\n",
      "Loss:0.5149518847465515\n",
      "Loss:0.40395691990852356\n",
      "Loss:0.2659316658973694\n",
      "Loss:0.520900547504425\n",
      "Loss:0.3632088005542755\n",
      "Loss:0.3739529550075531\n",
      "Loss:0.2996811866760254\n",
      "Loss:0.30063596367836\n",
      "Loss:0.5366112589836121\n",
      "Loss:0.3377710282802582\n",
      "Loss:0.3455989360809326\n",
      "Loss:0.5119988918304443\n",
      "Loss:0.2984890639781952\n",
      "Loss:0.43583759665489197\n",
      "Loss:0.4698716700077057\n",
      "Loss:0.5990787148475647\n",
      "Loss:0.6143754124641418\n",
      "Loss:0.2313300520181656\n",
      "Loss:0.3319035768508911\n",
      "Loss:0.4666275680065155\n",
      "Loss:0.4896279275417328\n",
      "Loss:0.31684502959251404\n",
      "Loss:0.2311834692955017\n",
      "Loss:0.45498260855674744\n",
      "Loss:0.3313794434070587\n",
      "Loss:0.29222190380096436\n",
      "Loss:0.35683703422546387\n",
      "Loss:0.3635197877883911\n",
      "Loss:0.1790451854467392\n",
      "Loss:0.17461690306663513\n",
      "Loss:0.29921939969062805\n",
      "Loss:0.31388548016548157\n",
      "Loss:0.23614844679832458\n",
      "Loss:0.22531947493553162\n",
      "Loss:0.3176785707473755\n",
      "Loss:0.2003324329853058\n",
      "Loss:0.2297976166009903\n",
      "Loss:0.6243269443511963\n",
      "Loss:0.5058987736701965\n",
      "Loss:0.417061448097229\n",
      "Loss:0.2723959684371948\n",
      "Loss:0.2402324378490448\n",
      "Loss:0.3870972692966461\n",
      "Loss:0.40097588300704956\n",
      "Loss:0.39880621433258057\n",
      "Loss:0.5818012952804565\n",
      "Loss:0.46556156873703003\n",
      "Loss:0.26666074991226196\n",
      "Loss:0.5742890238761902\n",
      "Loss:0.45386531949043274\n",
      "Loss:0.2641447186470032\n",
      "Loss:0.26195263862609863\n",
      "Loss:0.26300689578056335\n",
      "Loss:0.32853493094444275\n",
      "Loss:0.379037082195282\n",
      "Loss:0.34979113936424255\n",
      "Loss:0.5803713798522949\n",
      "Loss:0.48850905895233154\n",
      "Loss:0.4964674115180969\n",
      "Loss:0.4481165111064911\n",
      "Loss:0.4270000755786896\n",
      "Loss:0.5309339165687561\n",
      "Loss:0.6071101427078247\n",
      "Loss:0.36437150835990906\n",
      "Loss:0.2825015187263489\n",
      "Loss:0.484016478061676\n",
      "Loss:0.3221500515937805\n",
      "Loss:0.2536061108112335\n",
      "Loss:0.38475659489631653\n",
      "Loss:0.3304101526737213\n",
      "Loss:0.48712897300720215\n",
      "Loss:0.3711077570915222\n",
      "Loss:0.45943576097488403\n",
      "Loss:0.2989236116409302\n",
      "Loss:0.1909949630498886\n",
      "Loss:0.2751212418079376\n",
      "Loss:0.2511650025844574\n",
      "Loss:0.4568749666213989\n",
      "Loss:0.406032532453537\n",
      "Loss:0.3558901846408844\n",
      "Loss:0.29081615805625916\n",
      "Loss:0.4409068524837494\n",
      "Loss:0.26725366711616516\n",
      "Loss:0.3382677733898163\n",
      "Loss:0.5764263868331909\n",
      "Loss:0.42448362708091736\n",
      "Loss:0.34294068813323975\n",
      "Loss:0.3979857563972473\n",
      "Loss:0.22823910415172577\n",
      "Loss:0.16395732760429382\n",
      "Loss:0.36838722229003906\n",
      "Loss:0.2863028049468994\n",
      "Loss:0.3686427175998688\n",
      "Loss:0.34291601181030273\n",
      "Loss:0.26589491963386536\n",
      "Loss:0.19707690179347992\n",
      "Loss:0.34544551372528076\n",
      "Loss:0.3543718457221985\n",
      "Loss:0.34469300508499146\n",
      "Loss:0.32237425446510315\n",
      "Loss:0.179451584815979\n",
      "Loss:0.30929526686668396\n",
      "Loss:0.3554314076900482\n",
      "Loss:0.36457550525665283\n",
      "Loss:0.4107162058353424\n",
      "Loss:0.29638877511024475\n",
      "Loss:0.13914749026298523\n",
      "Loss:0.2964118719100952\n",
      "Loss:0.44885268807411194\n",
      "Loss:0.4607612192630768\n",
      "Loss:0.5471611618995667\n",
      "Loss:0.4069942533969879\n",
      "Loss:0.32310283184051514\n",
      "Loss:0.24998846650123596\n",
      "Loss:0.3158728778362274\n",
      "Loss:0.3487871289253235\n",
      "Loss:0.3177318871021271\n",
      "Loss:0.12433620542287827\n",
      "Loss:0.2737559676170349\n",
      "Loss:0.28661707043647766\n",
      "Loss:0.41652825474739075\n",
      "Loss:0.2730872333049774\n",
      "Loss:0.4358786940574646\n",
      "Loss:0.5818753242492676\n",
      "Loss:0.29544854164123535\n",
      "Loss:0.2782343626022339\n",
      "Loss:0.21985886991024017\n",
      "Loss:0.32737451791763306\n",
      "Loss:0.23772677779197693\n",
      "Loss:0.28188955783843994\n",
      "Loss:0.28576815128326416\n",
      "Loss:0.37746715545654297\n",
      "Loss:0.3651620149612427\n",
      "Loss:0.3215264678001404\n",
      "Loss:0.33819857239723206\n",
      "Loss:0.19301170110702515\n",
      "Loss:0.3777349293231964\n",
      "Loss:0.2285836637020111\n",
      "Loss:0.44080981612205505\n",
      "Loss:0.3966846764087677\n",
      "Loss:0.40132972598075867\n",
      "Loss:0.40494540333747864\n",
      "Loss:0.2597384452819824\n",
      "Loss:0.33859097957611084\n",
      "Loss:0.45328113436698914\n",
      "Loss:0.14483091235160828\n",
      "Loss:0.3313741385936737\n",
      "Loss:0.21890789270401\n",
      "Loss:0.4420170485973358\n",
      "Loss:0.3249306082725525\n",
      "Loss:0.2549964189529419\n",
      "Loss:0.2962155044078827\n",
      "Loss:0.3151986002922058\n",
      "Loss:0.5077126622200012\n",
      "Loss:0.434287965297699\n",
      "Loss:0.5903725624084473\n",
      "Loss:0.33187031745910645\n",
      "Loss:0.2798435091972351\n",
      "Loss:0.20433810353279114\n",
      "Loss:0.6078528165817261\n",
      "Loss:0.27463313937187195\n",
      "Loss:0.41713911294937134\n",
      "Loss:0.26055407524108887\n",
      "Loss:0.32072022557258606\n",
      "Loss:0.3485899865627289\n",
      "Loss:0.2564126253128052\n",
      "Loss:0.2676243782043457\n",
      "Loss:0.3615107536315918\n",
      "Loss:0.45689740777015686\n",
      "Loss:0.41575387120246887\n",
      "Loss:0.5555819869041443\n",
      "Loss:0.2626326084136963\n",
      "Loss:0.23180335760116577\n",
      "Loss:0.2881532907485962\n",
      "Loss:0.4732974171638489\n",
      "Loss:0.2903580069541931\n",
      "Loss:0.37063297629356384\n",
      "Loss:0.21880924701690674\n",
      "Loss:0.36039528250694275\n",
      "Loss:0.37893784046173096\n",
      "Loss:0.5182904005050659\n",
      "Loss:0.4564146399497986\n",
      "Loss:0.4774397611618042\n",
      "Loss:0.29597485065460205\n",
      "Loss:0.24955032765865326\n",
      "Loss:0.35870447754859924\n",
      "Loss:0.45932504534721375\n",
      "Loss:0.3726726770401001\n",
      "Loss:0.45559564232826233\n",
      "Loss:0.3621540367603302\n",
      "Loss:0.4872039258480072\n",
      "Loss:0.5851960182189941\n",
      "Loss:0.41053882241249084\n",
      "Loss:0.37727129459381104\n",
      "Loss:0.6439721584320068\n",
      "Loss:0.3211349546909332\n",
      "Loss:0.3600238263607025\n",
      "Loss:0.44300249218940735\n",
      "Loss:0.4256298243999481\n",
      "Loss:0.397675096988678\n",
      "Loss:0.5105741620063782\n",
      "Loss:0.16943581402301788\n",
      "Loss:0.3904663324356079\n",
      "Loss:0.4130273461341858\n",
      "Loss:0.2520933747291565\n",
      "Loss:0.38465186953544617\n",
      "Loss:0.25331786274909973\n",
      "Loss:0.4247705936431885\n",
      "Loss:0.386086106300354\n",
      "Loss:0.2740851640701294\n",
      "Loss:0.2612840235233307\n",
      "Loss:0.11620136350393295\n",
      "Loss:0.19832681119441986\n",
      "Loss:0.2568615972995758\n",
      "Loss:0.18946179747581482\n",
      "Loss:0.3652903735637665\n",
      "Loss:0.42487606406211853\n",
      "Loss:0.33998796343803406\n",
      "Loss:0.34346282482147217\n",
      "Loss:0.40604740381240845\n",
      "Loss:0.20137952268123627\n",
      "Loss:0.2648659646511078\n",
      "Loss:0.34111496806144714\n",
      "Loss:0.23357123136520386\n",
      "Loss:0.31638744473457336\n",
      "Loss:0.3094232380390167\n",
      "Loss:0.2468477040529251\n",
      "Loss:0.31819039583206177\n",
      "Loss:0.3767802119255066\n",
      "Loss:0.3273729085922241\n",
      "Loss:0.19115959107875824\n",
      "Loss:0.44283950328826904\n",
      "Loss:0.21881258487701416\n",
      "Loss:0.18538117408752441\n",
      "Loss:0.35846319794654846\n",
      "Loss:0.2457440346479416\n",
      "Loss:0.34181472659111023\n",
      "Loss:0.36503785848617554\n",
      "Loss:0.5323158502578735\n",
      "Loss:0.7483548521995544\n",
      "Loss:0.36454224586486816\n",
      "Loss:0.4540134370326996\n",
      "Loss:0.37999317049980164\n",
      "Loss:0.3117254078388214\n",
      "Loss:0.22706347703933716\n",
      "Loss:0.33079656958580017\n",
      "Loss:0.3727979063987732\n",
      "Loss:0.37510237097740173\n",
      "Loss:0.36420395970344543\n",
      "Loss:0.2849987745285034\n",
      "Loss:0.16487102210521698\n",
      "Loss:0.2589317262172699\n",
      "Loss:0.22945468127727509\n",
      "Loss:0.2821517586708069\n",
      "Loss:0.6626565456390381\n",
      "Loss:0.3793381154537201\n",
      "Loss:0.30815815925598145\n",
      "Loss:0.27942323684692383\n",
      "Loss:0.47109970450401306\n",
      "Loss:0.4111036956310272\n",
      "Loss:0.26965850591659546\n",
      "Loss:0.3543514013290405\n",
      "Loss:0.23371978104114532\n",
      "Loss:0.2877558469772339\n",
      "Loss:0.3433429002761841\n",
      "Loss:0.2564328610897064\n",
      "Loss:0.19777239859104156\n",
      "Loss:0.42082229256629944\n",
      "Loss:0.24189037084579468\n",
      "Loss:0.5251331925392151\n",
      "Loss:0.5810461044311523\n",
      "Loss:0.3910421133041382\n",
      "Loss:0.26979222893714905\n",
      "Loss:0.3911828398704529\n",
      "Loss:0.3346009850502014\n",
      "Loss:0.3197050094604492\n",
      "Loss:0.3108593821525574\n",
      "Loss:0.415016770362854\n",
      "Loss:0.4951110780239105\n",
      "Loss:0.35392242670059204\n",
      "Loss:0.35334134101867676\n",
      "Loss:0.39296308159828186\n",
      "Loss:0.4206881821155548\n",
      "Loss:0.42101818323135376\n",
      "Loss:0.2523624002933502\n",
      "Loss:0.1471261829137802\n",
      "Loss:0.16314882040023804\n",
      "Loss:0.3127373456954956\n",
      "Loss:0.266268789768219\n",
      "Loss:0.36015182733535767\n",
      "Loss:0.3717484176158905\n",
      "Loss:0.4016420543193817\n",
      "Loss:0.32952451705932617\n",
      "Loss:0.41712838411331177\n",
      "Loss:0.3272705078125\n",
      "Loss:0.1646701544523239\n",
      "Loss:0.24816294014453888\n",
      "Loss:0.31788796186447144\n",
      "Loss:0.4507516920566559\n",
      "Loss:0.3630114197731018\n",
      "Loss:0.2804781496524811\n",
      "Loss:0.25702187418937683\n",
      "Loss:0.5004111528396606\n",
      "Loss:0.38675177097320557\n",
      "Loss:0.30981168150901794\n",
      "Loss:0.4383965730667114\n",
      "Loss:0.46751126646995544\n",
      "Loss:0.39982667565345764\n",
      "Loss:0.4495609402656555\n",
      "Loss:0.48398956656455994\n",
      "Loss:0.18330565094947815\n",
      "Loss:0.2129950225353241\n",
      "Loss:0.3059861660003662\n",
      "Loss:0.2621297836303711\n",
      "Loss:0.29046374559402466\n",
      "Loss:0.2829655408859253\n",
      "Loss:0.4190378189086914\n",
      "Loss:0.3391018211841583\n",
      "Loss:0.30761823058128357\n",
      "Loss:0.33431485295295715\n",
      "Loss:0.25288572907447815\n",
      "Loss:0.469534695148468\n",
      "Loss:0.19488364458084106\n",
      "Loss:0.16282257437705994\n",
      "Loss:0.42146849632263184\n",
      "Loss:0.24920248985290527\n",
      "Loss:0.3104347586631775\n",
      "Loss:0.18360285460948944\n",
      "Loss:0.19224919378757477\n",
      "Loss:0.6993969678878784\n",
      "Loss:0.4415702223777771\n",
      "Loss:0.4330730140209198\n",
      "Loss:0.14105306565761566\n",
      "Loss:0.5295225977897644\n",
      "Loss:0.6052556037902832\n",
      "Loss:0.27767154574394226\n",
      "Loss:0.38024604320526123\n",
      "Loss:0.3094906806945801\n",
      "Loss:0.31861984729766846\n",
      "Loss:0.40245649218559265\n",
      "Loss:0.6936502456665039\n",
      "Loss:0.25329869985580444\n",
      "Loss:0.42877253890037537\n",
      "Loss:0.24962937831878662\n",
      "Loss:0.23452867567539215\n",
      "Loss:0.17144274711608887\n",
      "Loss:0.28222912549972534\n",
      "Loss:0.3617231249809265\n",
      "Loss:0.20320463180541992\n",
      "Loss:0.2474195808172226\n",
      "Loss:0.2282601296901703\n",
      "Loss:0.30681315064430237\n",
      "Loss:0.2788337469100952\n",
      "Loss:0.4431252181529999\n",
      "Loss:0.4901675283908844\n",
      "Loss:0.3950493335723877\n",
      "Loss:0.34479179978370667\n",
      "Loss:0.16936348378658295\n",
      "Loss:0.34012895822525024\n",
      "Loss:0.47172582149505615\n",
      "Loss:0.7395572662353516\n",
      "Loss:0.2872341573238373\n",
      "Loss:0.4187832176685333\n",
      "Loss:0.18969716131687164\n",
      "Loss:0.17973333597183228\n",
      "Loss:0.39058610796928406\n",
      "Loss:0.25707218050956726\n",
      "Loss:0.29401278495788574\n",
      "Loss:0.45639893412590027\n",
      "Loss:0.3617739677429199\n",
      "Loss:0.2383388876914978\n",
      "Loss:0.3065885007381439\n",
      "Loss:0.19620564579963684\n",
      "Loss:0.3110594153404236\n",
      "Loss:0.19537486135959625\n",
      "Loss:0.39405587315559387\n",
      "Loss:0.45927339792251587\n",
      "Loss:0.21008943021297455\n",
      "Loss:0.2885267436504364\n",
      "Loss:0.29339057207107544\n",
      "Loss:0.2794128656387329\n",
      "Loss:0.22699743509292603\n",
      "Loss:0.35428133606910706\n",
      "Loss:0.19814273715019226\n",
      "Loss:0.17030827701091766\n",
      "Loss:0.1836346834897995\n",
      "Loss:0.36193105578422546\n",
      "Loss:0.20233634114265442\n",
      "Loss:0.4072816073894501\n",
      "Loss:0.3640105724334717\n",
      "Loss:0.21092450618743896\n",
      "Loss:0.20710434019565582\n",
      "Loss:0.3309515416622162\n",
      "Loss:0.17010992765426636\n",
      "Loss:0.32657939195632935\n",
      "Loss:0.18767309188842773\n",
      "Loss:0.2948109805583954\n",
      "Loss:0.4357071816921234\n",
      "Loss:0.2725207507610321\n",
      "Loss:0.2603265941143036\n",
      "Loss:0.31197312474250793\n",
      "Loss:0.1658417284488678\n",
      "Loss:0.19859130680561066\n",
      "Loss:0.24925629794597626\n",
      "Loss:0.22809989750385284\n",
      "Loss:0.17532028257846832\n",
      "Loss:0.23885077238082886\n",
      "Loss:0.07511317729949951\n",
      "Loss:0.04249945655465126\n",
      "Loss:0.15898136794567108\n",
      "Loss:0.35296982526779175\n",
      "Loss:0.18944396078586578\n",
      "Loss:0.09304573386907578\n",
      "Loss:0.6106955409049988\n",
      "Loss:0.07635121792554855\n",
      "Loss:0.3684803545475006\n",
      "Loss:0.2907947897911072\n",
      "Loss:0.3280515670776367\n",
      "Loss:0.2065637856721878\n",
      "Loss:0.35198816657066345\n",
      "Loss:0.4314635992050171\n",
      "Loss:0.2906511127948761\n",
      "Loss:0.39988142251968384\n",
      "Loss:0.36015382409095764\n",
      "Loss:0.6273896098136902\n",
      "Loss:0.24479719996452332\n",
      "Loss:0.42310160398483276\n",
      "Loss:0.27917107939720154\n",
      "Loss:0.32116249203681946\n",
      "Loss:0.23327672481536865\n",
      "Loss:0.20538796484470367\n",
      "Loss:0.24389080703258514\n",
      "Loss:0.2440863698720932\n",
      "Loss:0.308258056640625\n",
      "Loss:0.30595001578330994\n",
      "Loss:0.1532377302646637\n",
      "Loss:0.31415724754333496\n",
      "Loss:0.30955979228019714\n",
      "Loss:0.310295432806015\n",
      "Loss:0.36070212721824646\n",
      "Loss:0.20169448852539062\n",
      "Loss:0.2846969664096832\n",
      "Loss:0.22951911389827728\n",
      "Loss:0.2799568474292755\n",
      "Loss:0.31362009048461914\n",
      "Loss:0.3423061966896057\n",
      "Loss:0.18265311419963837\n",
      "Loss:0.3061966300010681\n",
      "Loss:0.34532928466796875\n",
      "Loss:0.313859224319458\n",
      "Loss:0.21697860956192017\n",
      "Loss:0.22252482175827026\n",
      "Loss:0.29187867045402527\n",
      "Loss:0.35903531312942505\n",
      "Loss:0.3491306006908417\n",
      "Loss:0.29205313324928284\n",
      "Loss:0.4462418854236603\n",
      "Loss:0.30650627613067627\n",
      "Loss:0.2065277397632599\n",
      "Loss:0.2949235439300537\n",
      "Loss:0.29462578892707825\n",
      "Loss:0.36640143394470215\n",
      "Loss:0.1612892895936966\n",
      "Loss:0.2599329650402069\n",
      "Loss:0.21280768513679504\n",
      "Loss:0.16627299785614014\n",
      "Loss:0.3543899655342102\n",
      "Loss:0.16737280786037445\n",
      "Loss:0.3203965127468109\n",
      "Loss:0.4514642059803009\n",
      "Loss:0.3409948945045471\n",
      "Loss:0.22037504613399506\n",
      "Loss:0.4462542235851288\n",
      "Loss:0.30466845631599426\n",
      "Loss:0.31070074439048767\n",
      "Loss:0.25712791085243225\n",
      "Loss:0.252035528421402\n",
      "Loss:0.4708869159221649\n",
      "Loss:0.2771878242492676\n",
      "Loss:0.29379820823669434\n",
      "Loss:0.46400707960128784\n",
      "Loss:0.24299751222133636\n",
      "Loss:0.38857245445251465\n",
      "Loss:0.403809517621994\n",
      "Loss:0.556393027305603\n",
      "Loss:0.5533022284507751\n",
      "Loss:0.19503113627433777\n",
      "Loss:0.2869884669780731\n",
      "Loss:0.3910175561904907\n",
      "Loss:0.43123114109039307\n",
      "Loss:0.2789119482040405\n",
      "Loss:0.18853414058685303\n",
      "Loss:0.3786446750164032\n",
      "Loss:0.2860492169857025\n",
      "Loss:0.24795061349868774\n",
      "Loss:0.3187792897224426\n",
      "Loss:0.29504120349884033\n",
      "Loss:0.14567503333091736\n",
      "Loss:0.1276833713054657\n",
      "Loss:0.25177618861198425\n",
      "Loss:0.27804630994796753\n",
      "Loss:0.1904091238975525\n",
      "Loss:0.18563637137413025\n",
      "Loss:0.2592662572860718\n",
      "Loss:0.15643107891082764\n",
      "Loss:0.19328446686267853\n",
      "Loss:0.5435337424278259\n",
      "Loss:0.44001471996307373\n",
      "Loss:0.359146386384964\n",
      "Loss:0.2342841476202011\n",
      "Loss:0.18905694782733917\n",
      "Loss:0.32465043663978577\n",
      "Loss:0.33947136998176575\n",
      "Loss:0.32108569145202637\n",
      "Loss:0.5183315873146057\n",
      "Loss:0.4035968780517578\n",
      "Loss:0.22921781241893768\n",
      "Loss:0.488971471786499\n",
      "Loss:0.38522300124168396\n",
      "Loss:0.21903559565544128\n",
      "Loss:0.2140384316444397\n",
      "Loss:0.21693511307239532\n",
      "Loss:0.27955150604248047\n",
      "Loss:0.3259217441082001\n",
      "Loss:0.2966473698616028\n",
      "Loss:0.4964032769203186\n",
      "Loss:0.41537728905677795\n",
      "Loss:0.42272740602493286\n",
      "Loss:0.39125651121139526\n",
      "Loss:0.355267733335495\n",
      "Loss:0.443340539932251\n",
      "Loss:0.5369443297386169\n",
      "Loss:0.31036466360092163\n",
      "Loss:0.24256472289562225\n",
      "Loss:0.4339749217033386\n",
      "Loss:0.2799573540687561\n",
      "Loss:0.21649646759033203\n",
      "Loss:0.3289719820022583\n",
      "Loss:0.2830392122268677\n",
      "Loss:0.4122934639453888\n",
      "Loss:0.32377615571022034\n",
      "Loss:0.3978317379951477\n",
      "Loss:0.23646755516529083\n",
      "Loss:0.16610276699066162\n",
      "Loss:0.24108336865901947\n",
      "Loss:0.22843037545681\n",
      "Loss:0.39346832036972046\n",
      "Loss:0.3539411425590515\n",
      "Loss:0.29810023307800293\n",
      "Loss:0.23488542437553406\n",
      "Loss:0.36140158772468567\n",
      "Loss:0.21738573908805847\n",
      "Loss:0.28517183661460876\n",
      "Loss:0.5046247839927673\n",
      "Loss:0.36145883798599243\n",
      "Loss:0.31019505858421326\n",
      "Loss:0.36005252599716187\n",
      "Loss:0.1905653327703476\n",
      "Loss:0.13548643887043\n",
      "Loss:0.30071112513542175\n",
      "Loss:0.24896815419197083\n",
      "Loss:0.31430545449256897\n",
      "Loss:0.3049389123916626\n",
      "Loss:0.22977803647518158\n",
      "Loss:0.15360619127750397\n",
      "Loss:0.28714102506637573\n",
      "Loss:0.30456453561782837\n",
      "Loss:0.2930426001548767\n",
      "Loss:0.28104785084724426\n",
      "Loss:0.1481805145740509\n",
      "Loss:0.26806655526161194\n",
      "Loss:0.3080398440361023\n",
      "Loss:0.31682875752449036\n",
      "Loss:0.35984930396080017\n",
      "Loss:0.25966665148735046\n",
      "Loss:0.11105085164308548\n",
      "Loss:0.26027417182922363\n",
      "Loss:0.41674795746803284\n",
      "Loss:0.4068785309791565\n",
      "Loss:0.5240435600280762\n",
      "Loss:0.3681755065917969\n",
      "Loss:0.3002782464027405\n",
      "Loss:0.21758262813091278\n",
      "Loss:0.26310086250305176\n",
      "Loss:0.30275803804397583\n",
      "Loss:0.2709433138370514\n",
      "Loss:0.08774498850107193\n",
      "Loss:0.24706031382083893\n",
      "Loss:0.2227889448404312\n",
      "Loss:0.35812413692474365\n",
      "Loss:0.22749948501586914\n",
      "Loss:0.3631146252155304\n",
      "Loss:0.5452133417129517\n",
      "Loss:0.23725879192352295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36mtorch_train\u001b[0;34m(dataset, model, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, label\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 16\u001b[0m, in \u001b[0;36mTorchModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch_train(dataset, torch_model, 3e-4, 100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    y_pred = model(x.to(device))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "xd torch.Size([256, 784])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(x, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(x, model):\n\u001b[0;32m----> 2\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mTorchModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoppelganger/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x256)"
     ]
    }
   ],
   "source": [
    "print(predict(torch.tensor(x_test[0], dtype=torch.float32), torch_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyniki\n",
    "- Najlepszą dokładność na zbiorze walidacyjnym osiągnął model trzeci, z jedną ukrytą warstwą o rozmiarze 20, learning ratem o wartości 0.006 oraz batch sizem równym 128.\n",
    "- W modelu numer cztery w trakcie treningu doszło do przepełnienia.\n",
    "- Pozostałe modele osiągnęły dokładność na zbiorze walidacyjnym w granicach 80-85%.\n",
    "- Model trzeci na zbiorze testowym osiągnął wynik 93%.\n",
    "# Wnioski\n",
    "- Modele z wiekszą ilością ukrytych warstw oraz ukrytych neuronów nie zawsze osiągały wyższą dokładność. Architektura modelu powinna być ściśle dopasowana do poziomu złożoności danych - nie może być za prosta (jak w modelu 1) ani zbyt rozbudowana (jak w modelu 0).\n",
    "- Częste aktualizowanie parametrów w przypadku małego batch sizu może mieć negatywny wpływ na dokładność.\n",
    "- Mniejszy learning rate w modelu o bardziej rozbudowanej architekturze zapobiegał \"eksplozji\" gradientu.\n",
    "- Dokładność treningowa nie odbiegała zbytnio od dokładności walidacyjnej (największa różnica w modelu trzecim). Nie doszło do zjawiska zbytniego overfittingu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
